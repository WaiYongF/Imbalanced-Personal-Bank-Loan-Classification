{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":87370,"sourceType":"datasetVersion","datasetId":48024}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/fongwaiyong/imbalanced-personal-bank-loan-classification?scriptVersionId=208364164\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# <center> Imbalanced Bank Loan Modelling","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:130%; color:white; text-align:center; border-radius:10px 10px; padding:15px\">Step 1: Import Libraries</p>","metadata":{}},{"cell_type":"code","source":"pip install --upgrade visions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:23:54.815225Z","iopub.execute_input":"2024-11-19T07:23:54.815514Z","iopub.status.idle":"2024-11-19T07:24:05.160201Z","shell.execute_reply.started":"2024-11-19T07:23:54.815474Z","shell.execute_reply":"2024-11-19T07:24:05.158747Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nfrom ydata_profiling import ProfileReport\nfrom matplotlib.colors import ListedColormap, LinearSegmentedColormap\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import KBinsDiscretizer, OneHotEncoder, StandardScaler\nfrom sklearn.naive_bayes import ComplementNB, BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\nfrom sklearn.metrics import classification_report, RocCurveDisplay, ConfusionMatrixDisplay\nfrom scipy import stats\nfrom sklearn.base import clone \n\nimport warnings \nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:05.162779Z","iopub.execute_input":"2024-11-19T07:24:05.163311Z","iopub.status.idle":"2024-11-19T07:24:08.008536Z","shell.execute_reply.started":"2024-11-19T07:24:05.163272Z","shell.execute_reply":"2024-11-19T07:24:08.007207Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:130%; color:white; text-align:center; border-radius:10px 10px; padding:15px\">Step 2: Read Dataset</p>","metadata":{}},{"cell_type":"code","source":"original_df = pd.read_excel('/kaggle/input/bank-loan-modelling/Bank_Personal_Loan_Modelling.xlsx', \n                   sheet_name='Data')\ndf = original_df.copy()\ndf.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:08.009967Z","iopub.execute_input":"2024-11-19T07:24:08.010528Z","iopub.status.idle":"2024-11-19T07:24:08.963917Z","shell.execute_reply.started":"2024-11-19T07:24:08.010492Z","shell.execute_reply":"2024-11-19T07:24:08.962734Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:130%; color:white; text-align:center; border-radius:10px 10px; padding:15px\">Step 3: Dataset Univariate Analysis</p>","metadata":{}},{"cell_type":"code","source":"profile = ProfileReport(df, title=\"Data Profiling Report\")\nprofile.to_notebook_iframe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:08.965508Z","iopub.execute_input":"2024-11-19T07:24:08.966084Z","iopub.status.idle":"2024-11-19T07:24:32.124928Z","shell.execute_reply.started":"2024-11-19T07:24:08.966047Z","shell.execute_reply":"2024-11-19T07:24:32.123641Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h4 align=\"left\"><font color='royalblue'>Dataset basic information:</font></h4>\r\n\r\n* The dataset has data on __5000__ customers. \r\n* We have __14 variables__ including __13 independent variables__ and __1 dependent variable__ which is __Personal Loan__.\r\n* We have __6 numeric variables__: ID , Age , Experience ,  Income  , CC_Avg , Mortgage\r\n* We have __3 categorical variables__: Family , Education , Zip_Code\r\n* We have __5 Boolean variables__: Personal_Loan , Securities Account , CD_Account , Online , Credit_Card\r\n* There is no __missing value__ in the dataset.\r\n* There are no __duplicates__ in the dataset.\r\n* The dataset contains negative values for the __Experience__, which is unreasonable.\r\n* __ID__ is uniformly distributed. Therefore, ID acts as an identifier and lacks valuable information for the model.\r\n* __ZIP Code__ contains a large number of categories (467 categories). Therefore, it seems it lacks much informaton for our model.\r\n\r\n<h4 align=\"left\"><font color='royalblue'>Categorical Variables Analysis:</font></h4>\r\n\r\n* __Education -__ 42% of candidates have bachelor's degree and 30% have master's degree and 28% are professionals.\r\n* __Family -__ Around 29% of the customer's family size is 1, 26% is 2, 20% is 3 and 24% is 4.\r\n\r\n<h4 align=\"left\"><font color='royalblue'>Boolean Variables Analysis:</font></h4>\r\n\r\n* __Personal Loan -__ About 90% of the customers did not accept the personal loan offered in the last campaign. The dataset is __imbalanced__!\r\n* __CD Account -__ 94% of customers do not have a CD account with the bank.\r\n* __CreditCard -__ Around 71% of customers do not use credit cards.\r\n* __Online -__ Around 60% of customers use internet banking facilities.\r\n* __Securities Account -__ Around 90% of customers do not have a securities account with the bank.\r\n\r\n<h4 align=\"left\"><font color='royalblue'>Numerical Varibles Analysis:</font></h4>\r\n\r\n* __Age -__ The mean age of the customers is 45 with standard deviation of 11.5. The histogram curve is fairly symmetrical.\r\n* __CCAvg -__ The mean of average spending on credit cards per month is 1.94 with standard deviation of 1.75. The curve is highly positive skewed.\r\n* __Income -__ The mean annual income of the customer is 73.77 with standard deviation of 46. The curve is moderately positive skewed.\r\n* __Mortgage -__ The mean value of house mortgage is 56.5 with standard deviation of 101.71! The curve is highly positive skewed (Skewness = 2.1) and there are a lot of outliers (Kurtosis = 4.76)\r\n","metadata":{}},{"cell_type":"code","source":"df.drop('ID', axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:32.129078Z","iopub.execute_input":"2024-11-19T07:24:32.129585Z","iopub.status.idle":"2024-11-19T07:24:32.136828Z","shell.execute_reply.started":"2024-11-19T07:24:32.12954Z","shell.execute_reply":"2024-11-19T07:24:32.135408Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:130%; color:white; text-align:center; border-radius:10px 10px; padding:15px\">Step 4: Correlation Analysis</p>","metadata":{}},{"cell_type":"markdown","source":"Let's compare __Spearman__'s correlation with __Pearson__'s correlation:\r\n\r\n*  Pearson works with a linear relationship between the two variables whereas the Spearman works with monotonic relationships as well.\r\n* Pearson works with raw data values of the variables whereas Spearman works with rank-ordered variables.\r\n\r\n\r\nWhen the variables have a \"might be monotonic, might be linear\" relationship, our best bet is to use __Spearman__ rather than Pearson:","metadata":{}},{"cell_type":"code","source":"# Define a colormap\nroyalblue = LinearSegmentedColormap.from_list('royalblue', [(0, (1,1,1)), (1, (0.25,0.41,0.88))])\nroyalblue_r = royalblue.reversed()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:32.138759Z","iopub.execute_input":"2024-11-19T07:24:32.139225Z","iopub.status.idle":"2024-11-19T07:24:32.15405Z","shell.execute_reply.started":"2024-11-19T07:24:32.139188Z","shell.execute_reply":"2024-11-19T07:24:32.152766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%matplotlib inline\n\n# Calculation of the Spearman correlation\ntarget = 'Personal Loan'\ndf_ordered = pd.concat([df.drop(target,axis=1), df[target]],axis=1)\ncorr = df_ordered.corr(method='spearman')\n\n# Create a mask so that we see the correlation values only once\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask,1)] = True\n\n# Plot the heatmap correlation\nplt.figure(figsize=(12,8), dpi=80)\nsns.heatmap(corr, mask=mask, annot=True, cmap=royalblue, fmt='.2f', linewidths=0.2)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:32.155729Z","iopub.execute_input":"2024-11-19T07:24:32.156714Z","iopub.status.idle":"2024-11-19T07:24:32.732827Z","shell.execute_reply.started":"2024-11-19T07:24:32.156657Z","shell.execute_reply":"2024-11-19T07:24:32.731708Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h4 align=\"left\"><font color='midnightblue'>Conclusion:</font></h4>\r\n\r\n* __Personal Loan__ is highly correlated with __Income, CD_Account, CCAvg__.\r\n* __Experience__ is highly correlated with __Age__. (ρ = 0.99)\r\n* __CCAvg__ is correlated with __Income__ to a good extent. (ρ = 0.58)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:130%; color:white; text-align:center; border-radius:10px 10px; padding:15px\">Step 5: Data Cleansing</p>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 5.1: Noise Treatment</p>","metadata":{}},{"cell_type":"markdown","source":"__Noise treatment__ is the process by which irrelevant or noisy sections of a dataset are removed before the data can be used for analysis.","metadata":{}},{"cell_type":"markdown","source":"<h3 align=\"left\"><font color='tiffanyblue'>I) ZIP Code Noise Treatment:</font></h3>","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:32.734468Z","iopub.execute_input":"2024-11-19T07:24:32.734837Z","iopub.status.idle":"2024-11-19T07:24:32.742982Z","shell.execute_reply.started":"2024-11-19T07:24:32.734792Z","shell.execute_reply":"2024-11-19T07:24:32.741756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set the style of the graph\nsns.set_style('darkgrid')\ncolor = 'royalblue'\n\n# Plot histogram\nplt.figure(figsize=(15,5), dpi=120)\ngraph = sns.histplot(x='ZIP Code', data=df, bins=60, color=color)\n\n# Show non-zero values of the individual bars\nlabels = [str(v) if v else '' for v in graph.containers[0].datavalues]\ngraph.bar_label(graph.containers[0], labels=labels)\n\nplt.annotate('Noise', xy=(10000,60), xytext=(12000,400), color='red', fontsize=15, \n             arrowprops=dict(facecolor='red', shrink=0.01))\nplt.xlabel('ZIP Code', fontsize=15)\nplt.ylabel('Count', fontsize=15)\nplt.suptitle('ZIP Code Distribution', fontsize=20)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:32.744811Z","iopub.execute_input":"2024-11-19T07:24:32.745208Z","iopub.status.idle":"2024-11-19T07:24:33.49707Z","shell.execute_reply.started":"2024-11-19T07:24:32.74514Z","shell.execute_reply":"2024-11-19T07:24:33.495717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[df['ZIP Code']<20000]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:33.498692Z","iopub.execute_input":"2024-11-19T07:24:33.499802Z","iopub.status.idle":"2024-11-19T07:24:33.513003Z","shell.execute_reply.started":"2024-11-19T07:24:33.499764Z","shell.execute_reply":"2024-11-19T07:24:33.51201Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From the histogram, we found 1 noise data in the ZIP Code unexpectly. We drop the corresponding sample because it contains 4 digits, while the other values of this feature all have 5 digits:","metadata":{}},{"cell_type":"code","source":"df.drop(df[df['ZIP Code']<20000].index, inplace=True)\ndf.reset_index(drop=True, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:33.514488Z","iopub.execute_input":"2024-11-19T07:24:33.514864Z","iopub.status.idle":"2024-11-19T07:24:33.526606Z","shell.execute_reply.started":"2024-11-19T07:24:33.51483Z","shell.execute_reply":"2024-11-19T07:24:33.525423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h3 align=\"left\"><font color='tiffanyblue'>II) Experience Noise Treatment:</font></h3>","metadata":{}},{"cell_type":"markdown","source":"As seen in the third step, the dataset contains negative values for Experience. Considering that the values of this feature indicate work experience in years, these negative values are considered noise:","metadata":{}},{"cell_type":"code","source":"df[df['Experience']<0]['Experience'].count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:33.528072Z","iopub.execute_input":"2024-11-19T07:24:33.528444Z","iopub.status.idle":"2024-11-19T07:24:33.540662Z","shell.execute_reply.started":"2024-11-19T07:24:33.528414Z","shell.execute_reply":"2024-11-19T07:24:33.539413Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Count unique negative values:","metadata":{}},{"cell_type":"code","source":"df[df['Experience']<0]['Experience'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:33.542635Z","iopub.execute_input":"2024-11-19T07:24:33.542986Z","iopub.status.idle":"2024-11-19T07:24:33.557095Z","shell.execute_reply.started":"2024-11-19T07:24:33.542953Z","shell.execute_reply":"2024-11-19T07:24:33.555936Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since the number of these noise values in the Experience feature is small, we assume that these values are incorrectly recorded as negative and replace them with their absolute value:","metadata":{}},{"cell_type":"code","source":"# Apply absolute number\ndf['Experience'] = df['Experience'].apply(abs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:33.56257Z","iopub.execute_input":"2024-11-19T07:24:33.562987Z","iopub.status.idle":"2024-11-19T07:24:33.573572Z","shell.execute_reply.started":"2024-11-19T07:24:33.562949Z","shell.execute_reply":"2024-11-19T07:24:33.572237Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 5.2: Outlier Treatment</p>","metadata":{}},{"cell_type":"markdown","source":"__Outliers__ are data points that are significantly higher or lower than most other values in the set.\r\n\r\n__Kurtosis__ is a measure of the \"tailedness\" or shape of a distribution. If the Kurtosis value is greater than 3, it is likely that the variable contains outliers. This is because excessive kurtosis indicates that the data points have a higher concentration in the tails than normal, which may indicate the presence of outliers.\r\n\r\nIn 3d step, of all the continuous features, only the __Mortgage__ feature had a kurtosis value above 3. To detect possible outliers in this feature, we use __Z-score technique__.","metadata":{}},{"cell_type":"code","source":"sns.set(rc = {'axes.labelsize' : 15})               \nfig, ax = plt.subplots(1, 2, figsize=(15,5), dpi=120)\nsns.histplot(x='Mortgage', data=df, color='royalblue', ax=ax[0])\nsns.boxplot(x='Mortgage', data=df, color='royalblue', ax=ax[1])\nplt.suptitle('Mortgage Distribution', fontsize=20)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:33.575039Z","iopub.execute_input":"2024-11-19T07:24:33.575475Z","iopub.status.idle":"2024-11-19T07:24:34.347339Z","shell.execute_reply.started":"2024-11-19T07:24:33.575418Z","shell.execute_reply":"2024-11-19T07:24:34.346226Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The __Z-score__ method for outlier detection is a statistical technique used to detect outliers from data sets by calculating how many standard deviations away from the mean each data point is. A data point with a Z score of more than 3 standard deviation away from the mean is considered an outlier. We use the __scipy.stats__ module to perform the zscore technique:","metadata":{}},{"cell_type":"code","source":"df[stats.zscore(df['Mortgage'])>3]['Mortgage'].count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:34.348678Z","iopub.execute_input":"2024-11-19T07:24:34.349128Z","iopub.status.idle":"2024-11-19T07:24:34.364689Z","shell.execute_reply.started":"2024-11-19T07:24:34.34908Z","shell.execute_reply":"2024-11-19T07:24:34.361313Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We found 105 records with a Z-score mortgage value greater than 3. Therefore, we consider these 105 records as outliers and filter out these records from our dataset:","metadata":{}},{"cell_type":"code","source":"outlier_indexes = df[stats.zscore(df['Mortgage'])>3].index\ndf.drop(outlier_indexes, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:34.366293Z","iopub.execute_input":"2024-11-19T07:24:34.366756Z","iopub.status.idle":"2024-11-19T07:24:34.393737Z","shell.execute_reply.started":"2024-11-19T07:24:34.366718Z","shell.execute_reply":"2024-11-19T07:24:34.392674Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 5.3: Missing Value Treatment</p>","metadata":{}},{"cell_type":"markdown","source":"In the 3rd step, we found that our dataset does not contain any __missing values__. Now let us check again if there is any missing value:","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:34.395265Z","iopub.execute_input":"2024-11-19T07:24:34.395692Z","iopub.status.idle":"2024-11-19T07:24:34.405399Z","shell.execute_reply.started":"2024-11-19T07:24:34.395645Z","shell.execute_reply":"2024-11-19T07:24:34.40441Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The dataset does not contain any missing values.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 5.4: Duplicate Values Treatment</p>","metadata":{}},{"cell_type":"markdown","source":"__Duplicate Values Treatment__ is the process of removing duplicate records from the dataset before feeding them into a machine learning algorithm. This is to ensure that only unique samples are used to train and evaluate the machine learning algorithm:","metadata":{}},{"cell_type":"code","source":"df[df.duplicated(keep=False)].sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:34.406538Z","iopub.execute_input":"2024-11-19T07:24:34.406838Z","iopub.status.idle":"2024-11-19T07:24:34.431556Z","shell.execute_reply.started":"2024-11-19T07:24:34.40681Z","shell.execute_reply":"2024-11-19T07:24:34.430325Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 5.5: Feature Transformation</p>","metadata":{}},{"cell_type":"markdown","source":"In the dataset, CCAVG represents average monthly credit card spending, but Income represents the amount of annual income.\r\nTo make the units of the features equal, we convert average monthly credit card spending to annual:","metadata":{}},{"cell_type":"code","source":"df['CCAvg'] = df['CCAvg'] * 12","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:34.432732Z","iopub.execute_input":"2024-11-19T07:24:34.433057Z","iopub.status.idle":"2024-11-19T07:24:34.439329Z","shell.execute_reply.started":"2024-11-19T07:24:34.433026Z","shell.execute_reply":"2024-11-19T07:24:34.438066Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\r\nShow how many percent of dataset are remaining after data cleaning.","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:34.440949Z","iopub.execute_input":"2024-11-19T07:24:34.441733Z","iopub.status.idle":"2024-11-19T07:24:34.45451Z","shell.execute_reply.started":"2024-11-19T07:24:34.441669Z","shell.execute_reply":"2024-11-19T07:24:34.453297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Final dataset size after performing data preprocessing\n\ndf = df.copy()\n\nplt.title('Final Dataset')\nplt.pie([df.shape[0], original_df.shape[0]-df.shape[0]], radius=1, \n        labels=['Retained', 'Dropped'], counterclock=False, \n        autopct='%1.1f%%', pctdistance=0.9, explode=[0,0], shadow=True)\nplt.pie([df.shape[0]], labels=['100%'], labeldistance=-0, radius=0.78)\nplt.show()\n\nprint(f'\\033[1mInference:\\033[0m After the data preprocessing, {original_df.shape[0]-df.shape[0]} samples were dropped, \\\n    while retaining {round(100 - ((original_df.shape[0]-df.shape[0])*100/(original_df.shape[0])),2)}% of the data.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:34.456042Z","iopub.execute_input":"2024-11-19T07:24:34.456468Z","iopub.status.idle":"2024-11-19T07:24:34.626782Z","shell.execute_reply.started":"2024-11-19T07:24:34.456434Z","shell.execute_reply":"2024-11-19T07:24:34.625753Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:130%; color:white; text-align:center; border-radius:10px 10px; padding:15px\">Step 6: Bivariate Analysis</p>","metadata":{}},{"cell_type":"markdown","source":"Having considered each variable individually in univariate analysis in Step 3, we will now examine them again with respect to the __Target Variable__.\r\n\r\nFirst, we will identify the relationship between the __Target Variable__ and the __Categorical Features__. Then we will look at the __Numerical Features__ compared to the __Target Variable__.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 6.1: Categorical Features vs Target</p>","metadata":{}},{"cell_type":"markdown","source":"In this part, we will create __100% stacked bar and column charts__ showing the proportion of purchased and non-purchased loans for each category of categorical features separately:","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:34.628027Z","iopub.execute_input":"2024-11-19T07:24:34.628473Z","iopub.status.idle":"2024-11-19T07:24:34.647791Z","shell.execute_reply.started":"2024-11-19T07:24:34.628427Z","shell.execute_reply":"2024-11-19T07:24:34.646281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Cat_Features = ['Family', 'Education', 'Securities Account', 'CD Account', 'Online']\n\nTarget = 'Personal Loan'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:34.650058Z","iopub.execute_input":"2024-11-19T07:24:34.650668Z","iopub.status.idle":"2024-11-19T07:24:34.660874Z","shell.execute_reply.started":"2024-11-19T07:24:34.650591Z","shell.execute_reply":"2024-11-19T07:24:34.659554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=2, ncols= 3, figsize=(15,12), dpi=120)\n\nfor i, col in enumerate(Cat_Features):\n    cross_tab = pd.crosstab(index=df[col], columns=df[Target])\n\n    # Using the normalize=True argument gives us the index-wise proportiuon of the data\n    cross_tab_prop = pd.crosstab(index=df[col], columns=df[Target], normalize='index')\n\n    # Dfine colormap\n    cmp = ListedColormap(['royalblue', 'darkturquoise'])\n\n    # Plot stacked bar charts\n    x, y = i//3, i%3\n    cross_tab_prop.plot(kind='bar', ax=ax[x,y], stacked=True, width=0.8, \n                        colormap=cmp, legend=False, ylabel='Proportion', \n                        sharey=True)\n    \n     # Add the proportions and counts of the individual bars to our plot\n    for idx, val in enumerate([*cross_tab.index.values]):\n        for (proportion, count, y_location) in zip(cross_tab_prop.loc[val],cross_tab.loc[val],cross_tab_prop.loc[val].cumsum()):\n            ax[x,y].text(x=idx-0.22, y=(y_location-proportion)+(proportion/2)-0.03,\n                         s = f'  {count}\\n({np.round(proportion * 100, 1)}%)', \n                         color = \"black\", fontsize=9, fontweight=\"bold\")\n            \n    # Add legend\n    ax[x,y].legend(title='Personal Loan', loc=(0.7,0.9), fontsize=15, ncol=2)\n    #Set y limit\n    ax[x,y].set_ylim([0,1.12])\n    # Rotate xticks\n    ax[x,y].set_xticklabels(ax[x,y].get_xticklabels(), rotation=0)\n       \n\nplt.suptitle('Categorical Features vs Target Stacked Barplot' ,fontsize=20)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:34.662922Z","iopub.execute_input":"2024-11-19T07:24:34.664353Z","iopub.status.idle":"2024-11-19T07:24:36.245938Z","shell.execute_reply.started":"2024-11-19T07:24:34.66428Z","shell.execute_reply":"2024-11-19T07:24:36.244716Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h4 align=\"left\"><font color='midnightblue'>Conclusion:</font></h4>\r\n\r\n* The customer who has a certificate of deposit (CD) with the bank appears to buy personal loans from the bank.\r\n\r\n* Customers with higher levels of education are more likely to buy personal loans.\r\n\r\n* The number of family members has no significant effect on the probability of buying personal loans.\r\n\r\n* Customers who have or do not have a securities account at the bank have no influence on the probability of buying a personal loan.\r\n\r\n* The customer who uses or does not use internet banking does not seem to have any influence on the probability of buying a personal loan.\r\n\r\n* The customer who uses or does not use a credit card does not appear to have an impact on the likelihood of purchasing a personal loan.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 6.2: Numerical Features vs Target</p>","metadata":{}},{"cell_type":"markdown","source":"In this part, we will try to find the __mean__ and __distribution__ of numerical features for which customers purchase a personal loan versus the mean and distribution of numerical features who do not:","metadata":{}},{"cell_type":"code","source":"Num_Features = ['CCAvg','Income','Mortgage','Age','Experience']\n\nTarget = 'Personal Loan'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:36.247371Z","iopub.execute_input":"2024-11-19T07:24:36.247725Z","iopub.status.idle":"2024-11-19T07:24:36.253259Z","shell.execute_reply.started":"2024-11-19T07:24:36.247692Z","shell.execute_reply":"2024-11-19T07:24:36.25195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.set_palette(['royalblue', 'darkturquoise'])\n\nfig, ax = plt.subplots(5, 2, figsize=(15,15), dpi=200, gridspec_kw={'width_ratios': [1, 2]})\n\nfor i,col in enumerate(Num_Features):\n    # barplot\n    graph = sns.barplot(data=df, x=Target, y=col, ax=ax[i,0])\n    # kde Plot\n    sns.kdeplot(data=df[df[Target]==0], x=col, fill=True, linewidth=2, ax=ax[i,1], label='0')\n    sns.kdeplot(data=df[df[Target]==1], x=col, fill=True, linewidth=2, ax=ax[i,1], label='1')\n    ax[i,1].set_yticks([])\n    ax[i,1].legend(title='Personal Loan', loc='upper right')\n    # Add bar sizes to our plot\n    for cont in graph.containers:\n        graph.bar_label(cont, fmt='         %.3g')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:36.255382Z","iopub.execute_input":"2024-11-19T07:24:36.255771Z","iopub.status.idle":"2024-11-19T07:24:39.71534Z","shell.execute_reply.started":"2024-11-19T07:24:36.255738Z","shell.execute_reply":"2024-11-19T07:24:39.714197Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h4 align=\"left\"><font color='midnightblue'>Conclusion:</font></h4>\r\n\r\n* Customers who spend more on credit cards are more likely to take out personal loans.\r\n\r\n* Customers with high incomes are more likely to purchase a personal loan.\r\n\r\n* Customers with a high mortgage value are more likely to purchase personal loans.\r\n\r\n* It can be inferred that the age of customers has no influence on the probability of purchasing personal loans.\r\n\r\n* The effect of the amount of work experience on the purchase of a loan is similar to that of the age of the customer. The distribution of the Experience is very similar to the distribution of Age, as Experience is strongly correlated with Age.\r\n\r\n>Therefore, we remove Experience because it doesn't provide any more information than Age:","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:130%; color:white; text-align:center; border-radius:10px 10px; padding:15px\">Step 7: Train Test Split</p>","metadata":{}},{"cell_type":"markdown","source":"First of all, it is necessary to define the __features (X)__ and the output __labels (y)__ of the given dataset. X is a dataframe containing the different feature values for all given observations, while y is a series containing the categorical labels belonging to each observation:","metadata":{}},{"cell_type":"code","source":"X = df.drop('Personal Loan', axis=1)\ny = df['Personal Loan']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:39.716538Z","iopub.execute_input":"2024-11-19T07:24:39.716871Z","iopub.status.idle":"2024-11-19T07:24:39.723645Z","shell.execute_reply.started":"2024-11-19T07:24:39.716838Z","shell.execute_reply":"2024-11-19T07:24:39.722398Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"____\r\n<h2 align=\"left\"><font color='midnightblue'>Imbalanced dataset:</font></h2>\r\n\r\nImbalanced dataset is a dataset where the classes are not balanced or equal. This means there are an unequal number of samples from each target class and some classes may have significantly more samples than others. \r\n\r\nAs we saw in the 3d step, the dataset we are working on is an imbalanced dataset. Let's check it again:","metadata":{}},{"cell_type":"code","source":"plt.figure(dpi=80)\n\n# Plot frequency percentages horizon barplot\ndf['Personal Loan'].value_counts(normalize=True).mul(100).plot(kind='barh', width=0.8, figsize=(8,5))\n\n# Add frequency percentages to the plot\nlabels = df['Personal Loan'].value_counts(normalize=True).mul(100).round(1)\nfor i in labels.index:\n    plt.text(labels[i], i, str(labels[i])+ '%', fontsize=15, weight='bold')\n\nplt.xlim([0, 110])\nplt.xlabel('Frequency Percentage', fontsize=15)\nplt.ylabel('Personal Loan', fontsize=15)\nplt.title('Frequency Percentage of Target Classes', fontsize=15)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:39.725223Z","iopub.execute_input":"2024-11-19T07:24:39.725725Z","iopub.status.idle":"2024-11-19T07:24:39.963287Z","shell.execute_reply.started":"2024-11-19T07:24:39.725677Z","shell.execute_reply":"2024-11-19T07:24:39.962236Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It shows that __the dataset we are working on is imbalanced.__","metadata":{}},{"cell_type":"markdown","source":"____\r\n<h2 align=\"left\"><font color='midnightblue'>Problems with Imbalanced Datasets:</font></h2>\r\n\r\n1. Imbalanced datasets can lead to algorithms that are biased towards the majority class. This means that any classification algorithm trained on an imbalanced dataset will often inaccurately classify minority classes as the majority class.\r\n\r\n\r\n2. The performance of a machine learning model can become highly skewed when it is fed with imbalanced data. For example, our dataset has 91.2% class 0 and 8.8% class 1, then the learning model could be easily optimized to just predict all test input as belonging to class 0 and still get 91.2% accuracy!\r\n\r\n\r\n3. If a model is trained on data which is heavily imbalanced, it can develop an incorrect understanding of the underlying trends in the data. The model may not be able to recognize how certain values are distributed among different classes or even how certain classes overlap with each other.","metadata":{}},{"cell_type":"markdown","source":"____\r\n<h2 align=\"left\"><font color='midnightblue'>Techniques to Handle Imbalanced Dataset:</font></h2>\r\n\r\n<h3 align=\"left\"><font color='royalblue'>1. The approach to train test split:</font></h3>\r\n\r\nThe approach to train test split when dealing with __imbalanced datasets__ is to use __stratification__. Stratification is an important step in splitting imbalanced datasets into training and test sets. Stratification ensures that the proportion of each class remains the same across both the training and test sets. This is important as it allows for a more accurate evaluation of the model, avoiding bias due to a disproportion of one class being over-represented in either dataset. Stratified sampling also ensures that any trends or correlations among different classes that exist within the overall dataset are preserved after splitting into training and test datasets:","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:39.96453Z","iopub.execute_input":"2024-11-19T07:24:39.964878Z","iopub.status.idle":"2024-11-19T07:24:39.976232Z","shell.execute_reply.started":"2024-11-19T07:24:39.964845Z","shell.execute_reply":"2024-11-19T07:24:39.974782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_perc = pd.concat([y.value_counts(normalize=True).mul(100).round(1),\n                     y_train.value_counts(normalize=True).mul(100).round(1),\n                     y_test.value_counts(normalize=True).mul(100).round(1)],\n                     axis=1)\n\ndf_perc.columns = ['Dataset', 'Training', 'Test']\ndf_perc = df_perc.T\n\n# Plot frequency percentages barplot\ndf_perc.plot(kind='barh', stacked=True, figsize=(10,5), width=0.6)\n\n# Add the percentages to each plot\nfor idx, val in enumerate([*df_perc.index.values]):\n    for (percentage, y_location) in zip(df_perc.loc[val], df_perc.loc[val].cumsum()):\n        plt.text(x=(y_location - percentage) + (percentage / 2)-3,\n                 y=idx - 0.05,\n                 s=f'{percentage}%',\n                 color='black',\n                 fontsize=12,\n                 fontweight='bold')\n        \nplt.legend(title='Personal Loan', loc=(1.01,0.8))\nplt.xlabel('Frequency Precentage', fontsize=15)\nplt.title('Frequency Percentage of Target Classes among Training and Test Sets', fontsize=15)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:39.978237Z","iopub.execute_input":"2024-11-19T07:24:39.978622Z","iopub.status.idle":"2024-11-19T07:24:40.845584Z","shell.execute_reply.started":"2024-11-19T07:24:39.97858Z","shell.execute_reply":"2024-11-19T07:24:40.844343Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As seen, the samples are randomly divided in such a way that the proportion of each class remains the same across both the training and test sets. ","metadata":{}},{"cell_type":"markdown","source":"<h3 align=\"left\"><font color='royalblue'>2. The approach for model evaluation:</font></h3>\r\n\r\nThe approach for __model evaluation__ when dealing with __imbalanced datasets__ is to use appropriate metrics including __recall, precision, F1 score and AUC(area under curve)__ that are better suited when dealing with imbalanced datasets instead of traditional accuracy metrics which don’t take into account class imbalance by default.\r\n\r\nIn this project, the main goal is to classify potential customers who are more likely to purchase a loan. The metrics used to evaluate the performance of the model will be important in determining how well the model is able to identify these potential customers.\r\n\r\n* Recall is a measure of the proportion of actual positive cases that were correctly identified by the model. A high recall score means that the model has a low number of false negatives, which is desirable in this project because it means that the model is not missing many potential loan customers.\r\n\r\n* Precision is a measure of the proportion of positive cases identified by the model that are actually positive. A high precision score means that the model has a low number of false positives, which is desirable in this project because it means that the model is not identifying many non-loan customers as potential loan customers.\r\n\r\n* F1-score is a measure of the trade-off between recall and precision. It is calculated as the harmonic mean of recall and precision. A high F1-score indicates a balance between high recall and high precision.\r\n\r\n__For this project, both recall and precision for class '1' are important metrics, so f1-score for class '1' should be considered as the most important metric. A high f1-score indicates a balance between identifying as many potential loan customers as possible (high recall) and minimizing the number of false positives (high precision). This is important for the bank, as it wants to increase the conversion rate of depositors to borrowers while reducing the cost of the marketing campaign.__","metadata":{}},{"cell_type":"markdown","source":"<h3 align=\"left\"><font color='royalblue'>2. The approach for model evaluation:</font></h3>\r\n\r\nThe approach for __model evaluation__ when dealing with __imbalanced datasets__ is to use appropriate metrics including __recall, precision, F1 score and AUC(area under curve)__ that are better suited when dealing with imbalanced datasets instead of traditional accuracy metrics which don’t take into account class imbalance by default.\r\n\r\nIn this project, the main goal is to classify potential customers who are more likely to purchase a loan. The metrics used to evaluate the performance of the model will be important in determining how well the model is able to identify these potential customers.\r\n\r\n* Recall is a measure of the proportion of actual positive cases that were correctly identified by the model. A high recall score means that the model has a low number of false negatives, which is desirable in this project because it means that the model is not missing many potential loan customers.\r\n\r\n* Precision is a measure of the proportion of positive cases identified by the model that are actually positive. A high precision score means that the model has a low number of false positives, which is desirable in this project because it means that the model is not identifying many non-loan customers as potential loan customers.\r\n\r\n* F1-score is a measure of the trade-off between recall and precision. It is calculated as the harmonic mean of recall and precision. A high F1-score indicates a balance between high recall and high precision.\r\n\r\n__For this project, both recall and precision for class '1' are important metrics, so f1-score for class '1' should be considered as the most important metric. A high f1-score indicates a balance between identifying as many potential loan customers as possible (high recall) and minimizing the number of false positives (high precision). This is important for the bank, as it wants to increase the conversion rate of depositors to borrowers while reducing the cost of the marketing campaign.__","metadata":{}},{"cell_type":"markdown","source":"<h3 align=\"left\"><font color='royalblue'>3. Approaches for model building:</font></h3>\r\n\r\n> <h3 align=\"left\"><font color='midnightblue'>I) Data-based approaches:</font></h3>\r\n>\r\n>* __Random undersampling__: Undersampling involves randomly removing instances from the majority class to reduce its size.\r\n>    * __Cons__: Information loss caused by discarding the majority of the training set.\r\n>\r\n>\r\n>* __Random oversampling__: It involves adding additional copies of instances from the minority class to make it more equal in size.\r\n>    * __Cons__: Overfitting, which is caused by replicating observations from the minority class.\r\n> \r\n>\r\n>* __SMOTE__: SMOTE (Synthetic Minority Oversampling Technique) is an oversampling technique that creates new, synthetic observations from the minority class. This way, the algorithm avoids the problem of overfitting encountered with random oversampling.\r\n>\r\n>    * __Cons__: Since SMOTE randomly creates rows of new data, the newly created synthetic samples lack real value information, thus leading to a potential decrease in accuracy overall. In addition, if there are natural boundaries between classes such as overlapping classes, additional outliers may be created due to the artificial creation of data points through SMOTE.\r\n\r\n> <h3 align=\"left\"><font color='midnightblue'>II) Model-based approaches:</font></h3>\r\n>\r\n> * __Penalize Algorithms__: Penalize algorithms by giving more weight to a specific classification label, making correct classification of the minority class more important than correct classification of the majority class during optimisation process.\r\n>\r\n>\r\n> * __Use Tree-Based Algorithms__: Tree-based algorithms like Random Forest, Extra Trees Classifiers and XGBoost can naturally handle imbalanced data because they make decisions based on a majority vote among randomized decision trees and do not require balancing classes before training.\r\n","metadata":{}},{"cell_type":"markdown","source":"Considering the disadvantages of data-based methods, we will just implement model-based approaches.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:130%; color:white; text-align:center; border-radius:10px 10px; padding:15px\">Step 8: Feature Selection</p>","metadata":{}},{"cell_type":"markdown","source":"__Feature selection__ is the process of selecting a subset of relevant features for use in model building.","metadata":{}},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color='midnightblue'>Advantages of Feature Selection:</font></h2>\r\n\r\n1. __Enhanced Accuracy__: When there are fewer features, the model has fewer potential aspects to learn, which may lead to fewer errors being made when predicting new data.\r\n\r\n2. __Improved Interpretability__: Feature selection helps to identify important features and also make a model more interpretable by removing redundant or irrelevant features from the data set.\r\n\r\n3. __Faster Training Time__: Feature selection reduces the computational cost by reducing the number of computations that need to be performed in order to train and test the model. This, in turn, reduces training time and makes models more efficient.\r\n\r\n4. __Reduced Overfitting__: By removing irrelevant and redundant features from data, feature selection can also help in reducing overfitting which can occur when too many variables are included in a model without sufficient observations or regularization techniques applied.","metadata":{}},{"cell_type":"markdown","source":"____\r\n<h2 align=\"left\"><font color='midnightblue'>Feature Selection Methods:</font></h2>\r\n\r\n> <h3 align=\"left\"><font color='royalblue'>I) Wrapper Methods:</font></h3> \r\n> These methods use a predictive model to score each subset of features and determine the importance of each feature. Most important wrapper methods are: \r\n>\r\n> * __Forward Selection:__ Forward Selection is an iterative feature selection method that starts off with no features in the model and one by one adds the most predictive feature to the model, stopping when there is no improvement on a validation score. This process is repeated until certain criteria are met or all possible sets of features have been considered. By using a metric , the algorithm can identify which combinations of features best improve model performance. The result of Forward Selection is an ordered list that ranks the importance of each feature according to its contribution to improving model accuracy.\r\n>\r\n>\r\n> * __Permutation:__ Permutation evaluates the influence of a given feature by shuffling its values and thereby creating a random permutation, measuring how much the model accuracy drops as consequence. The greater the drop in accuracy, the more important this feature proved to be for solving the task at hand.\r\n>\r\n>\r\n> * __Drop-column:__ The idea is to calculate the model performance with all predictors and drop a single predictor and see the reduction in the performance. The more important the feature is, the larger the decrease we see in the model performance.\r\n\r\n\r\n> <h3 align=\"left\"><font color='royalblue'>II) Filter Methods:</font></h3> \r\n> These methods use statistical measures such as:\r\n>\r\n> * __Correlation Coefficients__\r\n> * __Information Gain__\r\n> * __Chi-square Test__\r\n\r\n\r\n> <h3 align=\"left\"><font color='royalblue'>III) Embedded Methods:</font></h3> \r\n> Embedded methods combine elements of both filter and wrapper approaches by constructing predictive models during the feature selection process in order to better evaluate potential features. Examples of embedded methods include: \r\n>\r\n> * __LASSO Regression__\r\n> * __Decision Trees__\r\n> * __Random Forest__\r\n> * __Gradient Boosting Machines (GBM)__","metadata":{}},{"cell_type":"markdown","source":"We will continue to use __Drop-column Feature Importance__ method as it is actually __the most accurate way__ to calculate the feature importances.\r\n\r\n__Note__: Drop-column method does not reflect to the intrinsic predictive value of a feature by itself but how important this feature is for __a particular model__. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 8.1: Drop-column Feature Importance Implementation</p>","metadata":{}},{"cell_type":"markdown","source":"As we discussed earlier, f1-score for calss '1' should be considered as the most important metric for this project:","metadata":{}},{"cell_type":"code","source":"# Define a scorer function\ndef f1_metric(model, X_train, y_train):\n    '''\n    This function reports f1-score metric for the class specified by 'Positive Label' (or '1')\n    '''\n    return f1_score(y_train, model.predict(X_train), average='binary')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:40.847679Z","iopub.execute_input":"2024-11-19T07:24:40.84823Z","iopub.status.idle":"2024-11-19T07:24:40.854547Z","shell.execute_reply.started":"2024-11-19T07:24:40.848177Z","shell.execute_reply":"2024-11-19T07:24:40.853381Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"__Cloning a model__ can be useful for creating multiple versions of the same model. This way, different versions of the model can be tested with various parameters or datasets. This can allow for more thorough and accurate evaluation of the model so that one version is not overly biased towards previous versions. Additionally it may be desirable to find out how slightly different parameters affect the performance of a model. Cloning a model allows us to do this quickly and easily by generating multiple models and testing them side-by-side. \r\n\r\nNext, we will implement Drop-column Feature Importance Technique through a function using cloning:","metadata":{}},{"cell_type":"code","source":"def drop_column_importance(model, X_train, y_train, random_state=0):\n    # list containing feature importances\n    importances = []\n    # Clone the model\n    model_clone = clone(model)\n    # Set random_state for comparability\n    model_clone.random_state = random_state\n    #Train the model\n    model_clone.fit(X_train, y_train)\n    # Create the cross validation object using StratifiedKFold\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n    # Score the benchmark model using cross-validation\n    benchmark_score = cross_val_score(model_clone, X_train, y_train, cv=cv, scoring =f1_metric).mean()\n\n    # Iterate over all features and store feature importance\n    for col in X_train.columns:\n        # Clone the model\n        model_clone = clone(model)\n        # Set random_state for comparability\n        model_clone.random_state = random_state\n        # Train the model on thje dataset with a single feature removed\n        model_clone.fit(X_train.drop(col, axis=1), y_train)\n        # Score the dropped-coloumn model\n        drop_column_score = cross_val_score(model_clone, X_train.drop(col, axis=1), y_train, cv=cv, scoring=f1_metric).mean()\n        # Store feature importance which is defined as the difference between the benchmark and the new model score\n        importances.append(benchmark_score - drop_column_score)\n\n    # Return the features along with theri importances in the form of a dataframe\n    importances_df = pd.DataFrame({'feature': X_train.columns, 'feature importance': importances}) \\\n                     .sort_values('feature importance', ascending = False).reset_index(drop = True)\n\n    return importances_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:40.855881Z","iopub.execute_input":"2024-11-19T07:24:40.85623Z","iopub.status.idle":"2024-11-19T07:24:40.873152Z","shell.execute_reply.started":"2024-11-19T07:24:40.856157Z","shell.execute_reply":"2024-11-19T07:24:40.871813Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Define a function to __visualize__ the results of Drop-column Feature Importance technique using bar charts:","metadata":{}},{"cell_type":"code","source":"def drop_column_importance_plot(model, X_train, y_train):\n    # Call drop-column feature importance funtion\n    df_drop_column = drop_column_importance(model, X_train, y_train, random_state=0)\n    # Rename columns\n    df_drop_column.columns = ['Feature', 'Feature Importance']\n\n    # Plot bar chart\n    plt.figure(figsize=(12,10))\n    sns.barplot(data=df_drop_column, x='Feature Importance', y='Feature', orient='h', color='royalblue')\n    plt.title('Drop Column Feature Importance', fontsize=20)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:40.874904Z","iopub.execute_input":"2024-11-19T07:24:40.875328Z","iopub.status.idle":"2024-11-19T07:24:40.898953Z","shell.execute_reply.started":"2024-11-19T07:24:40.875291Z","shell.execute_reply":"2024-11-19T07:24:40.897746Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the next steps, we will use these functions before building our final model regarding each classification algorithm to detect redundant features.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:130%; color:white; text-align:center; border-radius:10px 10px; padding:15px\">Step 9: Naive Bayes Model Building</p>","metadata":{}},{"cell_type":"markdown","source":"__Naive Bayes__ is a __classification__ algorithm in machine learning. It is used to predict the probability of a given input belonging to different classes or categories. It is based on Bayes' theorem, which uses the prior probability of the class and the likelihood of the features given the class to calculate the posterior probability of the class. The class with the highest posterior probability is then chosen as the predicted class for the input. The Naive Bayes classifier algorithm makes several assumptions about the data and the problem it is being used to solve.","metadata":{}},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color='midnightblue'>Naive Bayes Main Assumptions:</font></h2>\r\n\r\n1. __Independence:__ The algorithm assumes that all the features in the data are independent of each other given the class. This is the \"naive\" part of the algorithm and is often unrealistic in real-world problems, but it allows for a computationally efficient solution.\r\n\r\n2. __Conditional independence:__ The algorithm also assumes that the features are conditionally independent, meaning that the probability of a feature given the class is independent of the other features.\r\n\r\n3. __Constant class prior:__ The algorithm assumes that the class prior probabilities are constant and do not change with the data.\r\n\r\n__Note:__ The Naive Bayes basic assumptions have been satisfied since only two features were highly correlated, Experience and Age, and the Experience feature had been removed earlier.","metadata":{}},{"cell_type":"markdown","source":"_____\r\n<h2 align=\"left\"><font color='midnightblue'>Different types of Naive Bayes algorithms available in sklearn:</font></h2>\r\n\r\n* __Gaussian Naive Bayes:__ This algorithm is used when the data is continuous and follows a __normal__ distribution.\r\n\r\n* __Multinomial Naive Bayes:__ This algorithm is used when the data is discrete and represents the __count of occurrences__ of each category.\r\n\r\n* __Bernoulli Naive Bayes:__ This algorithm is similar to the multinomial Naive Bayes, but it is used when the data is __binary__. \r\n\r\n* __Complement Naive Bayes:__ This algorithm is similar to the Multinomial Naive Bayes, but it is designed for __imbalanced data sets__. \r\n\r\n* __Categorical Naive Bayes:__ This algorithm is similar to the Multinomial Naive Bayes, but it is designed for __categorical data, rather than count data__. \r\n\r\n\r\nIn this project, our dataset contains a mixture of features with different distributions:\r\n* Continuous Features - Age, Income, CCAvg, Mortgage\r\n* Binary Features - Securities Account, CD Account, Online, CreditCard\r\n* Multinomial Features - Family\r\n* Categorical Features - Education, ZIP Code\r\n\r\n> <h3 align=\"left\"><font color='midnightblue'>Strategies:</font></h3> \r\n>\r\n> * __First strategy__ is to independently fit a Gaussian NB model on the continuous part of the data, a Complement NB model (imbalanced dataset) on the multinomial part of the data, a Bernoulli model on the bernoulli part of the data and a Categorical NB on the categorical part of the data. After fitting each model on the corresponding part of the dataset, then we actually transform the dataset by taking the class assignment probabilities (using predict_proba method) as new features and then refit a new Gaussian NB model on the new features.\r\n>\r\n> * __Second strategy__ is to discretize continuous features and apply different discrete-data based Naive Bayes models, including Complement NB, and Bernoulli NB, in order to find the model with the best performance.\r\n\r\n\r\n>We will implement the __second strategy__ because Income, CCAvg, and Mortgage are continuous features that are important features according to step 6.2, but their distribution is not normal and has high skewness and kurtosis, which causes a large error when performing Gaussian NB.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 9.1: Complement NB Model Building</p>","metadata":{}},{"cell_type":"markdown","source":"__Multinomial NB__ is a probabilistic classifier that is commonly used in text classification tasks. It is based on the assumption that the features (e.g. words) in the text are conditionally independent given the class label. Multinomial NB models the probability of each feature given a class label using a multinomial distribution. \r\n\r\n__Complement NB__ is a variant of Multinomial NB algorithm that is designed to correct for the bias that is inherent in Multinomial NB algorithm. Multinomial NB algorithm tends to assign higher probability to classes that have more training samples. Complement NB aims to correct for this bias by computing the complement of the standard Naive Bayes probability estimates and then using these complement probabilities to make predictions. \r\n\r\nSince our dataset is imbalanced, we will use CNB instead of MNB.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:70%; color:white; text-align:center; border-radius:10px 10px; padding:6px\">Step 9.1.1: Complement NB Feature Discretization</p>","metadata":{}},{"cell_type":"markdown","source":"__KBinsDiscretizer__ class from scikit-learn provides an implementation of discretization using the binning method. It allows us to choose between different number of bins (n_bins) and strategies for binning. \r\n\r\nWe will define a function containing __GridSearchCV__ class to find the best combination of __n_bins__ and __strategy__. In other words, we try all combinations of n_bins and strategy within the defined range and use the considered NB model to evaluate the discretizer's performance on the validation set in order to find the optimal combination:","metadata":{}},{"cell_type":"code","source":"def discretization_report(df, clf):\n    '''\n    This function finds the optimal combination of n_bins and strategy for continuous features discretization\n    '''\n    # Define continuous features to perform discretization on\n    cols_to_discretize = ['Age', 'Income', 'CCAvg', 'Mortgage']\n\n    # Define the features (X) and the output label (y)\n    X = df[cols_to_discretize]\n    y = df['Personal Loan']\n\n    # Split dataset into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n\n    # Define the grid search parameters\n    param_grid = {'discretizer__strategy': ['uniform', 'quantile', 'kmeans'],\n                  'discretizer__n_bins': np.arange(2,11)}\n    \n    # Define the KBinsDiscretizer and ONeHotEncoder and ComplementNB objects\n    discretizer = KBinsDiscretizer(encode='ordinal')\n    onehot = OneHotEncoder(handle_unknown='ignore', drop='first')\n\n    # Create the pipeline\n    pipeline = Pipeline([('discretizer', discretizer), ('onehot', onehot), ('clf', clf)])\n\n    # Create the cross-validation object using StartifiedKFold to ensure the class distribution is the same across all the folds\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n\n    # Create the GridSearchCV object\n    grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='f1')\n    \n    # Fit the GridSearchCV object to the training data\n    grid_search.fit(X_train, y_train)\n\n    # Print the best parameters and the best score\n    print(\"Best discretization parameters:\", grid_search.best_params_)\n    print(\"Best score:\", grid_search.best_score_)\n    \n    # Return optimal values for n_bins and strategy\n    return grid_search.best_params_['discretizer__n_bins'], grid_search.best_params_['discretizer__strategy']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:40.900284Z","iopub.execute_input":"2024-11-19T07:24:40.900619Z","iopub.status.idle":"2024-11-19T07:24:40.918234Z","shell.execute_reply.started":"2024-11-19T07:24:40.900588Z","shell.execute_reply":"2024-11-19T07:24:40.916746Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Find optimal values for KBinsDiscretizer parameters using discretization_report function:","metadata":{}},{"cell_type":"code","source":"# Ignore a warning that in some cases, the width of the bins will be too small\nwarnings.simplefilter(action='ignore')\n\n# Initialize the CNB classifier\ncnb = ComplementNB()\n\n# Call discretization_report\nn_bins, strategy = discretization_report(df, cnb)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:40.919794Z","iopub.execute_input":"2024-11-19T07:24:40.920315Z","iopub.status.idle":"2024-11-19T07:24:43.757452Z","shell.execute_reply.started":"2024-11-19T07:24:40.920265Z","shell.execute_reply":"2024-11-19T07:24:43.756235Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The optimal values obtained for n_bins and strategy considering the CNB model are:\r\n* n_bins : 6\r\n* strategy : quantile (All bins in each feature have the same number of points)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:70%; color:white; text-align:center; border-radius:10px 10px; padding:6px\">Step 9.1.2: Complement NB Feature Encoding</p>","metadata":{}},{"cell_type":"markdown","source":"After obtaining the optimal values for KBinsDiscretizer arguments including n_bins and strategy, we discretize continuous features with these optimal arguments. Then, we implement dummy encoding on non-binary categorical features. We define a function for feature encoding: ","metadata":{}},{"cell_type":"code","source":"def nb_feature_encoding(df, n_bins, strategy, cols_to_encode):\n    '''\n    This function performs dummy encoding on the desired categorical features after performing feature discretization\n    considering optimal n_bins and strategy values.\n    '''\n    # Define continuous features to perform discretization on\n    cols_to_discretize = ['Age', 'Income', 'CCAvg', 'Mortgage']\n\n    # Define the feature (X) and the output label (y)\n    X = df.drop('Personal Loan', axis=1)\n    y = df['Personal Loan']\n\n    # Split the dataset into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n\n    # Discretize the continuous features\n    discretizer = KBinsDiscretizer(n_bins=n_bins, strategy=strategy, encode='ordinal')\n    X_train[cols_to_discretize] = discretizer.fit_transform(X_train[cols_to_discretize])\n    X_test[cols_to_discretize] = discretizer.transform(X_test[cols_to_discretize])\n\n    # OneHot Encode the discretized features\n    X_train = pd.get_dummies(X_train, columns=cols_to_encode, drop_first=True)\n    X_test = pd.get_dummies(X_test, columns=cols_to_encode, drop_first=True)\n\n    # Align the columns of the test set with the training set\n    X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n    \n    # Return transformed features\n    return X_train, X_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:43.758795Z","iopub.execute_input":"2024-11-19T07:24:43.759123Z","iopub.status.idle":"2024-11-19T07:24:43.76733Z","shell.execute_reply.started":"2024-11-19T07:24:43.759092Z","shell.execute_reply":"2024-11-19T07:24:43.766048Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Applying onehot encoding on the desired features using nb_feature_encoding function:","metadata":{}},{"cell_type":"code","source":"# All non-binary features are selected for onehot encoding\ncols_to_encode = ['Age', 'Income', 'CCAvg', 'Mortgage', 'Family', 'Education']\n\n# Call nb_feature_encoding\nX_train, X_test = nb_feature_encoding(df, n_bins, strategy, cols_to_encode)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:43.768939Z","iopub.execute_input":"2024-11-19T07:24:43.770037Z","iopub.status.idle":"2024-11-19T07:24:43.80952Z","shell.execute_reply.started":"2024-11-19T07:24:43.769994Z","shell.execute_reply":"2024-11-19T07:24:43.808289Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:70%; color:white; text-align:center; border-radius:10px 10px; padding:6px\">Step 9.1.3: Complement NB Feature Subset Selection</p>","metadata":{}},{"cell_type":"markdown","source":"To find the most important features considering CNB model, we use the drop_column_importance_plot function we defined earlier:","metadata":{}},{"cell_type":"code","source":"# Initialize the CNB classifier\ncnb = ComplementNB()\n\n# Call drop_column_importance_plot\ndrop_column_importance_plot(cnb, X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:43.811246Z","iopub.execute_input":"2024-11-19T07:24:43.811714Z","iopub.status.idle":"2024-11-19T07:24:45.764141Z","shell.execute_reply.started":"2024-11-19T07:24:43.811667Z","shell.execute_reply":"2024-11-19T07:24:45.762947Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Negative importance in Drop-column Feature Importance means that removing the corresponding feature from the model actually improves the model performance. So we filter our dataset:","metadata":{}},{"cell_type":"code","source":"feature_importances = drop_column_importance(cnb, X_train, y_train, 0)\nselected_features = feature_importances[feature_importances['feature importance']>0]['feature']\n\n# Filter dataset\nX_train = X_train[selected_features]\nX_test = X_test[selected_features]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:45.765671Z","iopub.execute_input":"2024-11-19T07:24:45.76622Z","iopub.status.idle":"2024-11-19T07:24:47.16358Z","shell.execute_reply.started":"2024-11-19T07:24:45.766154Z","shell.execute_reply":"2024-11-19T07:24:47.162407Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:70%; color:white; text-align:center; border-radius:10px 10px; padding:6px\">Step 9.1.4: Complement NB Model Building</p>","metadata":{}},{"cell_type":"markdown","source":"After removing irrelevant features, we train the final CNB model:","metadata":{}},{"cell_type":"code","source":"cnb=ComplementNB()\ncnb.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:47.172732Z","iopub.execute_input":"2024-11-19T07:24:47.173136Z","iopub.status.idle":"2024-11-19T07:24:47.18653Z","shell.execute_reply.started":"2024-11-19T07:24:47.173101Z","shell.execute_reply":"2024-11-19T07:24:47.185364Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:70%; color:white; text-align:center; border-radius:10px 10px; padding:6px\">Step 9.1.5: Complement NB Model Evaluation</p>","metadata":{}},{"cell_type":"markdown","source":"To evaluate the performance of the model, we define a function so that it can be used to evaluate subsequent models as well:","metadata":{}},{"cell_type":"code","source":"def metrics_calculator(clf, X_test, y_test, model_name):\n    '''\n    This function calculates all desired performance metrics for a given model on test data.\n    '''\n    y_pred = clf.predict(X_test)\n    result = pd.DataFrame(data=[accuracy_score(y_test, y_pred),\n                                precision_score(y_test, y_pred, average='binary'),\n                                recall_score(y_test, y_pred, average='binary'),\n                                f1_score(y_test, y_pred, average='binary'),\n                                roc_auc_score(y_test, clf.predict_proba(X_test)[::,1])],\n                          index=['Accuracy','Precision','Recall','F1-score','AUC'],\n                          columns = [model_name])\n    \n    result = (result * 100).round(2).astype(str) + '%'                            \n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:47.187736Z","iopub.execute_input":"2024-11-19T07:24:47.188217Z","iopub.status.idle":"2024-11-19T07:24:47.195961Z","shell.execute_reply.started":"2024-11-19T07:24:47.188145Z","shell.execute_reply":"2024-11-19T07:24:47.194727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def model_evaluation(clf, X_train, X_test, y_train, y_test, model_name):\n    '''\n    This function provides a complete report of the model's performance including classification reports, \n    confusion matrix and ROC curve.\n    '''\n    sns.set(font_scale=1.2)\n    \n    # Generate classification report for training set\n    y_pred_train = clf.predict(X_train)\n    print(\"\\n\\t  Classification report for training set\")\n    print(\"-\"*55)\n    print(classification_report(y_train, y_pred_train))\n\n    # Generate classification report for test set\n    y_pred_test = clf.predict(X_test)\n    print(\"\\n\\t   Classification report for test set\")\n    print(\"-\"*55)\n    print(classification_report(y_test, y_pred_test))\n    \n    # Create figure and subplots \n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5), dpi=100, gridspec_kw={'width_ratios': [2, 2, 1]})\n    \n    # Plot confusion matrix for test set\n    ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test, colorbar=False, cmap=royalblue_r, ax=ax1)\n    ax1.set_title('Confusion Matrix for Test Data')                                     \n    ax1.grid(False)\n    \n    # Plot ROC curve for test data and display AUC score \n    RocCurveDisplay.from_estimator(clf, X_test, y_test, ax=ax2)\n    ax2.set_xlabel('False Positive Rate')\n    ax2.set_ylabel('True Positive Rate')\n    ax2.set_title('ROC Curve for Test Data (Positive label: 1)')\n    \n    # Report results for the class specified by positive label\n    result = metrics_calculator(clf, X_test, y_test, model_name)\n    table = ax3.table(cellText=result.values, colLabels=result.columns, rowLabels=result.index, loc='center')\n    table.scale(0.6, 2)\n    table.set_fontsize(12)\n    ax3.axis('tight')\n    ax3.axis('off')\n    # Modify color \n    for key, cell in table.get_celld().items():\n        if key[0] == 0:\n            cell.set_color('royalblue')\n    plt.tight_layout()\n    plt.show() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:47.197626Z","iopub.execute_input":"2024-11-19T07:24:47.19798Z","iopub.status.idle":"2024-11-19T07:24:47.210938Z","shell.execute_reply.started":"2024-11-19T07:24:47.197948Z","shell.execute_reply":"2024-11-19T07:24:47.210007Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's call the above function for evaluating our CNB model:","metadata":{}},{"cell_type":"code","source":"model_evaluation(cnb, X_train, X_test, y_train, y_test, 'Complement NB')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:47.212354Z","iopub.execute_input":"2024-11-19T07:24:47.212693Z","iopub.status.idle":"2024-11-19T07:24:48.170991Z","shell.execute_reply.started":"2024-11-19T07:24:47.212662Z","shell.execute_reply":"2024-11-19T07:24:48.169758Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We got a recall score of about 94% from Complement NB, which is good, but due to the low precision value of 38%, the f1-score is about 54%.","metadata":{}},{"cell_type":"code","source":"# Save the final performance of Complement Naive Bayes classifier\ncnb_result = metrics_calculator(cnb, X_test, y_test, 'Complement Naive Bayes')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:48.172228Z","iopub.execute_input":"2024-11-19T07:24:48.172533Z","iopub.status.idle":"2024-11-19T07:24:48.193232Z","shell.execute_reply.started":"2024-11-19T07:24:48.172503Z","shell.execute_reply":"2024-11-19T07:24:48.192256Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 9.2: Bernoulli NB Model Building</p>","metadata":{}},{"cell_type":"markdown","source":"__Bernoulli NB__, like MultinomialNB, is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed to work with binary data, where the features are either true or false (1 or 0).","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:70%; color:white; text-align:center; border-radius:10px 10px; padding:6px\">Step 9.2.1: Bernoulli NB Feature Discretization</p>","metadata":{}},{"cell_type":"code","source":"# Initialize the Categorical NB classifier\nbnb = BernoulliNB()\n\n# Call discretization_report\nn_bins, strategy = discretization_report(df, bnb)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:48.194474Z","iopub.execute_input":"2024-11-19T07:24:48.19477Z","iopub.status.idle":"2024-11-19T07:24:50.993869Z","shell.execute_reply.started":"2024-11-19T07:24:48.194741Z","shell.execute_reply":"2024-11-19T07:24:50.992439Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The optimal values obtained for n_bins and strategy considering the BNB model are:\r\n* n_bins : 9\r\n* strategy : uniform (All bins in each feature have identical widths)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:70%; color:white; text-align:center; border-radius:10px 10px; padding:6px\">Step 9.2.2: Bernoulli NB Feature Encoding</p>","metadata":{}},{"cell_type":"markdown","source":"Now, we discretize continuous features with the obtained optimal n_bins and strategy. Then, we need to implement dummy encoding on all non-binary features. ","metadata":{}},{"cell_type":"markdown","source":"Since the ZIP Code feature contains a large number of categories and is not an important feature, we will remove it for Bernoulli NB modeling:","metadata":{}},{"cell_type":"code","source":"df_bnb = df.copy()\ndf_bnb.drop('ZIP Code', axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:50.995269Z","iopub.execute_input":"2024-11-19T07:24:50.995596Z","iopub.status.idle":"2024-11-19T07:24:51.002943Z","shell.execute_reply.started":"2024-11-19T07:24:50.995564Z","shell.execute_reply":"2024-11-19T07:24:51.001565Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Applying onehot encoding on all non-binary features using nb_feature_encoding function:","metadata":{}},{"cell_type":"code","source":"# All non-binary features are selected for onehot encoding\ncols_to_encode = ['Age', 'Income', 'CCAvg', 'Mortgage', 'Family', 'Education']\n\n# Call nb_feature_encoding\nX_train, X_test = nb_feature_encoding(df_bnb, n_bins, strategy, cols_to_encode)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:51.004542Z","iopub.execute_input":"2024-11-19T07:24:51.004954Z","iopub.status.idle":"2024-11-19T07:24:51.042358Z","shell.execute_reply.started":"2024-11-19T07:24:51.00492Z","shell.execute_reply":"2024-11-19T07:24:51.041228Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:70%; color:white; text-align:center; border-radius:10px 10px; padding:6px\">Step 9.2.3: Bernoulli NB Feature Subset Selection</p>","metadata":{}},{"cell_type":"markdown","source":"To find the most important features considering BNB model, we again use the drop_column_importance_plot function we defined earlier:","metadata":{}},{"cell_type":"code","source":"# Initialize the BNB classifier\nbnb = BernoulliNB()\n\n# Call drop_column_importance_plot\ndrop_column_importance_plot(bnb, X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:51.043791Z","iopub.execute_input":"2024-11-19T07:24:51.044243Z","iopub.status.idle":"2024-11-19T07:24:54.498419Z","shell.execute_reply.started":"2024-11-19T07:24:51.044195Z","shell.execute_reply":"2024-11-19T07:24:54.497094Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Again negative importance in Drop-column Feature Importance means that removing the corresponding feature from the model actually improves the model performance. So we filter our dataset:","metadata":{}},{"cell_type":"code","source":"# Find Important features with positive feature_importance value\nfeature_importances = drop_column_importance(bnb, X_train, y_train, 0)\nselected_features = feature_importances[feature_importances['feature importance']>0]['feature']\n\n# Filter dataset\nX_train = X_train[selected_features]\nX_test = X_test[selected_features]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:54.500065Z","iopub.execute_input":"2024-11-19T07:24:54.50046Z","iopub.status.idle":"2024-11-19T07:24:57.338317Z","shell.execute_reply.started":"2024-11-19T07:24:54.500426Z","shell.execute_reply":"2024-11-19T07:24:57.337341Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:70%; color:white; text-align:center; border-radius:10px 10px; padding:6px\">Step 9.2.4: Bernoulli NB Model Building</p>","metadata":{}},{"cell_type":"markdown","source":"After removing irrelevant features, we train the final BNB model:","metadata":{}},{"cell_type":"code","source":"bnb = BernoulliNB()\nbnb.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:57.339445Z","iopub.execute_input":"2024-11-19T07:24:57.339761Z","iopub.status.idle":"2024-11-19T07:24:57.354365Z","shell.execute_reply.started":"2024-11-19T07:24:57.339731Z","shell.execute_reply":"2024-11-19T07:24:57.353255Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:70%; color:white; text-align:center; border-radius:10px 10px; padding:6px\">Step 9.2.5: Bernoulli NB Model Evaluation</p>","metadata":{}},{"cell_type":"markdown","source":"Evaluate our trained Bernoulli NB model performance using model_evaluation function:","metadata":{}},{"cell_type":"code","source":"model_evaluation(bnb, X_train, X_test, y_train, y_test, 'Bernoulli NB')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:57.356015Z","iopub.execute_input":"2024-11-19T07:24:57.35639Z","iopub.status.idle":"2024-11-19T07:24:58.280804Z","shell.execute_reply.started":"2024-11-19T07:24:57.356357Z","shell.execute_reply":"2024-11-19T07:24:58.279616Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We got a 5% increase in f1-score compared to Complement NB model. In the Bernoulli NB model, compared to Complement NB, the value of precision has increased, while the value of recall has decreased. AUC value did not differ.","metadata":{}},{"cell_type":"code","source":"# Save the final performance of Bernoulli Naive Bayes classifier\nbnb_result = metrics_calculator(bnb, X_test, y_test, 'Bernoulli Naive Bayes')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:58.282635Z","iopub.execute_input":"2024-11-19T07:24:58.283131Z","iopub.status.idle":"2024-11-19T07:24:58.304716Z","shell.execute_reply.started":"2024-11-19T07:24:58.283083Z","shell.execute_reply":"2024-11-19T07:24:58.303779Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:130%; color:white; text-align:center; border-radius:10px 10px; padding:15px\">Step 10: Logistic Regression Model Building</p>","metadata":{}},{"cell_type":"markdown","source":"__Logistic Regression__ is a type of supervised machine learning algorithm used for binary classification problems. It models the probability of a target variable (usually binary) as a function of input features, using a logistic function (sigmoid) to map predictions between 0 and 1. The model is trained using labeled data to optimize the coefficients of the features to minimize the prediction error.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 10.1: Scale Data using Standard Scaler</p>","metadata":{}},{"cell_type":"markdown","source":"Before building our logistic classifier, we will apply __Standard Scaler__ to our data.\r\n\r\n__Standard Scaler__ is used to scale the data. It transforms the data by subtracting the mean and dividing by the standard deviation, ensuring that all features have a similar range of values.\r\n\r\n\r\n<h3 align=\"left\"><font color='midnightblue'>Benefits of Standard Scaling on Logistic Regression:</font></h3>\r\n\r\n1. __Gradient Descent Convergence__: The optimization algorithm used in logistic regression is gradient descent. When the features have different scales, the magnitude of the gradient will also be different for different features. Scaling the features to have the same scale ensures that the magnitude of the gradient is the same for all features, which can lead to faster convergence of the optimization algorithm.\r\n\r\n\r\n2. __Regularization__: Logistic regression uses regularization to prevent overfitting. When the features have different scales, the regularization term in the cost function tends to give more weight to the features with higher values, which can be problematic in certain cases. Scaling the features to have the same scale can alleviate this issue.\r\n\r\n\r\n3. __Better Performance__: In some cases, scaling the features can lead to a better performance of the logistic regression model, especially when the features have a skewed distribution or are not on the same scale.","metadata":{}},{"cell_type":"code","source":"# Perform train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:58.305938Z","iopub.execute_input":"2024-11-19T07:24:58.306313Z","iopub.status.idle":"2024-11-19T07:24:58.316136Z","shell.execute_reply.started":"2024-11-19T07:24:58.306279Z","shell.execute_reply":"2024-11-19T07:24:58.315034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Scale the training and test data using the same scaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Convert training and test sets from numpy array to pandas dataframes\nX_train = pd.DataFrame(X_train_scaled, columns=X_train.columns)\nX_test = pd.DataFrame(X_test_scaled, columns=X_test.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:24:58.317682Z","iopub.execute_input":"2024-11-19T07:24:58.318189Z","iopub.status.idle":"2024-11-19T07:24:58.33334Z","shell.execute_reply.started":"2024-11-19T07:24:58.318108Z","shell.execute_reply":"2024-11-19T07:24:58.332203Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"__Note:__ It is very important that StandardScaler transformation should only be learnt from the training set, otherwise it will lead to data leakage.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 10.2: Logistic Regression Hyperparameter Tuning</p>","metadata":{}},{"cell_type":"markdown","source":"__Hyperparameter tuning__ can affect the performance of a logistic regression model by allowing it to find the best combination of hyperparameters that result in the lowest error on the training set. This can lead to improved prediction performance and reduced overfitting. However, if not done, it can lead to overfitting on the validation set, resulting in a model that is not generalizable to new data.\r\n\r\nAlso, since our dataset is imbalanced, we intend to make the correct classification of the minority class more important than the correct classification of the majority class in the optimization process, which is called __penalizing__ the model. We do this by giving more weight to the minority class. Therefore, the weights of the classes are hyperparameters whose optimal value is also determined during the Hyperparameter tuning process.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:70%; color:white; text-align:center; border-radius:10px 10px; padding:6px\">Step 10.2.1: Define the Hyperparameter Grid</p>","metadata":{}},{"cell_type":"markdown","source":"Hyperparameter Grid refers to a pre-defined set of hyperparameters to be tested in a model training process. Each combination of hyperparameters is a single point in the grid, and the goal is to select the best hyperparameters for the model by evaluating model performance on a validation set. The grid defines the search space for hyperparameter optimization algorithms to find the optimal hyperparameters.","metadata":{}},{"cell_type":"markdown","source":"In Logistic Regression, solver choice is determined by penalty choice. Supported penalties by solvers are:\r\n\r\n> * __lbfgs -> [ l2 , None ]__\r\n>\r\n> * __liblinear -> [ l1 , l2 ]__\r\n>\r\n> * __newton-cg -> [ l2 , None ]__\r\n>\r\n> * __sag -> [ l2 , None ]__\r\n>\r\n> * __saga -> [ elasticnet , l1 , l2 , None ]__\r\n\r\n\r\nTherefore, different combinations of solver and penalty should be considered:","metadata":{}},{"cell_type":"code","source":"# Weights associated with classes\nclass_weights = [{0:x, 1:1.0-x} for x in np.linspace(0.001,0.5,20)]\n\n# Define hyperparameters grid\nparam_grid = [{'solver':['lbfgs', 'newton-cg', 'sag', 'saga'], \n               'penalty':['None'], \n               'class_weight':class_weights}, \n              \n              {'solver':['lbfgs', 'newton-cg', 'sag'], \n               'penalty':['l2'], \n               'C': np.logspace(-5, 5, 10), \n               'class_weight':class_weights},\n              \n              {'solver':['liblinear', 'saga'], \n               'penalty': ['l1', 'l2'], \n               'C': np.logspace(-5, 5, 10), \n               'class_weight':class_weights},\n              \n              {'solver':['saga'], \n               'penalty':['elasticnet'], \n               'C': np.logspace(-5, 5, 10),\n               'l1_ratio': np.arange(0,1.1,0.1), \n               'class_weight':class_weights}]    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:57:41.930649Z","iopub.execute_input":"2024-11-19T07:57:41.931151Z","iopub.status.idle":"2024-11-19T07:57:41.941453Z","shell.execute_reply.started":"2024-11-19T07:57:41.931102Z","shell.execute_reply":"2024-11-19T07:57:41.940425Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:70%; color:white; text-align:center; border-radius:10px 10px; padding:6px\">Step 10.2.2: Find the Optimal Hyperparameters</p>","metadata":{}},{"cell_type":"markdown","source":"We use GridSearchCV to find the optimal combination of hyperparameters that gives the best performance on the training data:","metadata":{}},{"cell_type":"markdown","source":"We will define a function in which the optimal combination of hyperparameters that will cause the best __f1-score__ value for the model will be discovered. For this reason, we define a function so that it can be used to tune the hyperparameters of future models as well:","metadata":{}},{"cell_type":"code","source":"def tune_clf_hyperparameters(clf, param_grid, X_train, y_train):\n    '''\n    This function optimize the hyperparameters for a classifier by searching over a specified hyperparameter grid. It uses \n    GridSearchCV and cross-validation (StratifiedKFold) to evaluate different combinations of hyperparameters, and selects  \n    the combination with the highest f1-score. The function returns the best classifier with the optimal hyperparameters.\n    '''\n    \n    # Create the cross-validation object using StratifiedKFold to ensure the class distribution is the same across all the folds\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n\n    # Create the GridSearchCV object\n    clf_grid = GridSearchCV(clf, param_grid, cv=cv, scoring=f1_metric, n_jobs=-1)\n\n    # Fit the GridSearchCV object to the training data\n    clf_grid.fit(X_train, y_train)\n\n    # Get the best hyperparameters\n    print(\"Best hyperparameters:\\n\", clf_grid.best_params_)\n    \n    # Return best_estimator_ attribute which gives us the best model that has been fitted to the training data\n    return clf_grid.best_estimator_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:57:45.949795Z","iopub.execute_input":"2024-11-19T07:57:45.950194Z","iopub.status.idle":"2024-11-19T07:57:45.957896Z","shell.execute_reply.started":"2024-11-19T07:57:45.950142Z","shell.execute_reply":"2024-11-19T07:57:45.95631Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We use the above function to find the optimal combination of hyperparameters for logistic regression classifier:","metadata":{}},{"cell_type":"code","source":"# Define the base model\nlogreg = LogisticRegression(max_iter=5000)\n\n# Call tune_clf_hyperparameters function to find the optimal combination of hyperparameters \nlogreg_opt = tune_clf_hyperparameters(logreg, param_grid, X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:07:43.662044Z","iopub.execute_input":"2024-11-19T08:07:43.662558Z","iopub.status.idle":"2024-11-19T08:22:41.91887Z","shell.execute_reply.started":"2024-11-19T08:07:43.662512Z","shell.execute_reply":"2024-11-19T08:22:41.917454Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"These are the optimal hyperparameter values for the Logistic Regression model. logreg_opt is the Logistic Regression model whose hyperparameters have been set to the optimal values.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 10.3: Logistic Regression Feature Subset Selection</p>","metadata":{}},{"cell_type":"markdown","source":"Let us check how important each of the features is for our logistic regression model. We use the drop_column_importance_plot function that we have defined earlier:","metadata":{}},{"cell_type":"code","source":"drop_column_importance_plot(logreg_opt, X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:26:31.911605Z","iopub.execute_input":"2024-11-19T08:26:31.912057Z","iopub.status.idle":"2024-11-19T08:26:34.084808Z","shell.execute_reply.started":"2024-11-19T08:26:31.912018Z","shell.execute_reply":"2024-11-19T08:26:34.083734Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Almost no negative values are observed among Drop-column Feature Importances. In other words, all features are effective in target estimation.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 10.4: Logistic Regression Model Evaluation</p>","metadata":{}},{"cell_type":"markdown","source":"Let's evaluate the Logistic Regression model performance using model_evaluation function:","metadata":{}},{"cell_type":"code","source":"model_evaluation(logreg_opt, X_train, X_test, y_train, y_test, 'Logistic Regression')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:26:34.087103Z","iopub.execute_input":"2024-11-19T08:26:34.08757Z","iopub.status.idle":"2024-11-19T08:26:35.092783Z","shell.execute_reply.started":"2024-11-19T08:26:34.087523Z","shell.execute_reply":"2024-11-19T08:26:35.091321Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A 10% increase in the F1-score of the logistic model compared to the NB Bernoulli model is observed, which is wonderful.","metadata":{}},{"cell_type":"code","source":"# Save the final performance of Logistic Regression classifier\nlogreg_result = metrics_calculator(logreg_opt, X_test, y_test, 'Logistic Regression')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:130%; color:white; text-align:center; border-radius:10px 10px; padding:15px\">Step 11: KNN Model Building</p>","metadata":{}},{"cell_type":"markdown","source":"__KNN (K-Nearest Neighbors)__ is a supervised learning algorithm used for classification and regression problems in machine learning. The algorithm works by finding the K nearest data points to a given test sample, and then classifying the test sample based on the majority class among the K nearest neighbors. The algorithm uses a distance metric (such as Euclidean distance) to determine the nearest neighbors. The value of K is a hyperparameter and determines the number of neighbors used to make the prediction.\r\n\r\n<h4 align=\"left\"><font color='midnightblue'>Advantages:</font></h4>\r\n\r\n* Easy to implement and understand.\r\n* No need for training, it saves the training data and does not need to estimate parameters.\r\n* Can be used for both classification and regression problems.\r\n\r\n<h4 align=\"left\"><font color='midnightblue'>Disadvantages:</font></h4>\r\n\r\n* It requires a large amount of memory to store the training data.\r\n* Computationally expensive during prediction stage.\r\n* Does not work well with high dimensional data as the distance metric becomes less effective.\r\n* It can be biased towards the majority class.\r\n* Sensitive to outliers.\r\n* Sensitive to irrelevant features and noisy data. It's important to perform proper feature selection.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 11.1: Scale Data using Standard Scaler</p>","metadata":{}},{"cell_type":"markdown","source":"As KNN uses a distance metric to find its nearest neighbors, standard scaling (or normalization) is necessary to rescale the features so they have the same scale. Features with different scales and distributions can affect the distance calculation and affect the algorithm's performance.","metadata":{}},{"cell_type":"code","source":"# Perform train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:26:35.094882Z","iopub.execute_input":"2024-11-19T08:26:35.095911Z","iopub.status.idle":"2024-11-19T08:26:35.113457Z","shell.execute_reply.started":"2024-11-19T08:26:35.095828Z","shell.execute_reply":"2024-11-19T08:26:35.111877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Scale the training and test data using the same scaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Convert training and test sets from numpy array to pandas dataframes\nX_train = pd.DataFrame(X_train_scaled, columns=X_train.columns)\nX_test = pd.DataFrame(X_test_scaled, columns=X_test.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:26:35.117231Z","iopub.execute_input":"2024-11-19T08:26:35.117822Z","iopub.status.idle":"2024-11-19T08:26:35.141207Z","shell.execute_reply.started":"2024-11-19T08:26:35.117741Z","shell.execute_reply":"2024-11-19T08:26:35.139792Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 11.2: KNN Hyperparameter Tuning</p>","metadata":{}},{"cell_type":"markdown","source":"KNN classifier hyperparameters are as follows:\r\n\r\n>* __n_neighbors:__ This is the number of nearest neighbors that will be used to predict the class of a new sample. \r\n>\r\n>* __weights:__ This determines how the distances between samples are weighted when making predictions. \r\n>    * uniform - all neighbors are weighted equally\r\n>    * distance - neighbors closer to the sample are weighted more heavily\r\n>\r\n>* __metric:__ This is the distance metric used to determine the closest neighbors. The options are: \r\n>    * euclidean\r\n>    * manhattan\r\n>    * minkowski (the generalization of both distances)\r\n>\r\n>* __p:__ This is the power parameter for the Minkowski metric. When p=1, the Minkowski metric is equivalent to the Manhattan distance; when p=2, it is equivalent to the Euclidean distance. A value of p other than 1 or 2 can be used to weight the contribution of the distances between coordinates differently.","metadata":{}},{"cell_type":"markdown","source":"We need to tune the value of hyperparameters for our knn classifier. For this purpose, first we define our hyperparameters grid, and then call tune_clf_hyperparameters function to find the best combination of hyperparameters:","metadata":{}},{"cell_type":"code","source":"# Define hyperparameters grid to search\nparam_grid = [{'n_neighbors': np.arange(2, 30), 'metric': ['euclidean','manhattan'], 'weights': ['uniform']},\n              {'n_neighbors': np.arange(2, 30), 'metric': ['minkowski'], 'p': [3,4,5], 'weights': ['uniform']}]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:26:35.142249Z","iopub.execute_input":"2024-11-19T08:26:35.142521Z","iopub.status.idle":"2024-11-19T08:26:35.147701Z","shell.execute_reply.started":"2024-11-19T08:26:35.142494Z","shell.execute_reply":"2024-11-19T08:26:35.146606Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's call tune_clf_hyperparameters function for hyperparameter tuning:","metadata":{}},{"cell_type":"code","source":"# Create a KNN classifier object\nknn = KNeighborsClassifier()\n\n# Find the best classifier with the optimal hyperparameters\nknn_opt = tune_clf_hyperparameters(knn, param_grid, X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:26:35.148772Z","iopub.execute_input":"2024-11-19T08:26:35.14948Z","iopub.status.idle":"2024-11-19T08:28:06.37299Z","shell.execute_reply.started":"2024-11-19T08:26:35.149444Z","shell.execute_reply":"2024-11-19T08:28:06.371896Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 11.3: KNN Feature Subset Selection</p>","metadata":{}},{"cell_type":"markdown","source":"KNN classifiers are sensitive to irrelevant features because they measure distances between instances and weigh each feature equally in the distance calculation. This means that a feature with a high level of randomness or noise can have a large influence on the calculated distances, resulting in inaccurate predictions:","metadata":{}},{"cell_type":"code","source":"drop_column_importance_plot(knn_opt, X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:28:06.374452Z","iopub.execute_input":"2024-11-19T08:28:06.374896Z","iopub.status.idle":"2024-11-19T08:28:25.092401Z","shell.execute_reply.started":"2024-11-19T08:28:06.374861Z","shell.execute_reply":"2024-11-19T08:28:25.091071Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the Drop-Column feature importance output results, several features with negative importance are observed. Removing these features improves the performance of the model. One of these features with the most negative importance value is the ZIP Code, which we already realized in the 3d step, that it is an unimportant feature due to its large number of categories. \r\nWe filter our dataset:","metadata":{}},{"cell_type":"code","source":"# Find Important features with positive feature_importance value\nfeature_importances = drop_column_importance(knn_opt, X_train, y_train, 0)\nselected_features = feature_importances[feature_importances['feature importance']>0]['feature']\n\n# Filter dataset\nX_train = X_train[selected_features]\nX_test = X_test[selected_features]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:28:25.093728Z","iopub.execute_input":"2024-11-19T08:28:25.094195Z","iopub.status.idle":"2024-11-19T08:28:43.465557Z","shell.execute_reply.started":"2024-11-19T08:28:25.094116Z","shell.execute_reply":"2024-11-19T08:28:43.4645Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's tune the hyperparameters of our model again after removing the irrelevant features:","metadata":{}},{"cell_type":"code","source":"# Create a KNN classifier object\nknn = KNeighborsClassifier()\n\n# Find the best classifier with the optimal hyperparameters\nknn_opt = tune_clf_hyperparameters(knn, param_grid, X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:28:43.466901Z","iopub.execute_input":"2024-11-19T08:28:43.467253Z","iopub.status.idle":"2024-11-19T08:29:08.223473Z","shell.execute_reply.started":"2024-11-19T08:28:43.46722Z","shell.execute_reply":"2024-11-19T08:29:08.222248Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 11.4: KNN Model Evaluation</p>","metadata":{}},{"cell_type":"markdown","source":"Let's evaluate the KNN model performance using model_evaluation function:","metadata":{}},{"cell_type":"code","source":"model_evaluation(knn_opt, X_train, X_test, y_train, y_test, 'KNN')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:29:08.227961Z","iopub.execute_input":"2024-11-19T08:29:08.22833Z","iopub.status.idle":"2024-11-19T08:29:09.759052Z","shell.execute_reply.started":"2024-11-19T08:29:08.228298Z","shell.execute_reply":"2024-11-19T08:29:09.757764Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We got an astonishing 91% F1-score, 95% precision and 87% recall using the simple KNN classifier!","metadata":{}},{"cell_type":"code","source":"# Save the final performance of KNN classifier\nknn_result = metrics_calculator(knn_opt, X_test, y_test, 'K-Nearest Neighbors')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:29:09.760664Z","iopub.execute_input":"2024-11-19T08:29:09.761289Z","iopub.status.idle":"2024-11-19T08:29:09.892851Z","shell.execute_reply.started":"2024-11-19T08:29:09.761237Z","shell.execute_reply":"2024-11-19T08:29:09.891928Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:130%; color:white; text-align:center; border-radius:10px 10px; padding:15px\">Step 12: SVM Model Building</p>","metadata":{}},{"cell_type":"markdown","source":"__Support Vector Machine (SVM)__ is a type of supervised learning algorithm that is used for classification or regression tasks. It works by finding the best boundary (also known as the decision boundary) that separates the data points into classes, while maximizing the margin (the distance between the boundary and the closest data points from each class, known as support vectors). SVM can handle non-linearly separable data by transforming it into a higher-dimensional space, where it becomes linearly separable.\r\n\r\n<h4 align=\"left\"><font color='midnightblue'>Advantages:</font></h4>\r\n\r\n\r\n* __Robust to outliers:__ SVM is less sensitive to outliers compared to other algorithms, making it suitable for tasks where the presence of outliers is expected.\r\n\r\n* __Versatile:__ SVM can be used for classification and regression tasks, as well as handling non-linear data through the use of kernel functions.\r\n\r\n* __Effective in high dimensional spaces:__ SVM is effective in higher dimensional spaces, where the number of features is greater than the number of samples. \r\n\r\n* __Good performance on smaller datasets:__ SVM has a good performance even on smaller datasets and it doesn't require a large amount of training data to produce accurate results.\r\n\r\n\r\n<h4 align=\"left\"><font color='midnightblue'>Disadvantages:</font></h4>\r\n\r\n* __Poor performance on large datasets:__ SVM can be computationally intensive and may become slow when the dataset is large.\r\n\r\n* __Overfitting:__ SVM can overfit the data, especially when the number of features is much higher than the number of samples.\r\n\r\n* __Limited interpretability:__ SVM is a black-box model and it can be difficult to interpret the results and understand how the algorithm makes predictions.\r\n\r\n* __Difficulty in choosing the right kernel function:__ The choice of the kernel function is crucial for the performance of SVM and it can be difficult to determine the best one for a particular problem.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 12.1: Scale Data using Standard Scaler</p>","metadata":{}},{"cell_type":"markdown","source":"SVM (Support Vector Machine) is a distance-based classifier. SVM works by finding the boundary that best separates the data into classes, while maximizing the margin (the distance between the boundary and the closest data points from each class, known as support vectors). This means that SVM is based on the concept of distance and seeks to find the optimal boundary that maximizes the distance between the classes. Therefore, Standard Scaling must be performed before model building:","metadata":{}},{"cell_type":"code","source":"# Perform train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:29:09.893998Z","iopub.execute_input":"2024-11-19T08:29:09.894336Z","iopub.status.idle":"2024-11-19T08:29:09.904957Z","shell.execute_reply.started":"2024-11-19T08:29:09.894305Z","shell.execute_reply":"2024-11-19T08:29:09.903968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Scale the training and test data using the same scaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Convert training and test sets from numpy array to pandas dataframes\nX_train = pd.DataFrame(X_train_scaled, columns=X_train.columns)\nX_test = pd.DataFrame(X_test_scaled, columns=X_test.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:29:09.906106Z","iopub.execute_input":"2024-11-19T08:29:09.906499Z","iopub.status.idle":"2024-11-19T08:29:09.919961Z","shell.execute_reply.started":"2024-11-19T08:29:09.906456Z","shell.execute_reply":"2024-11-19T08:29:09.918994Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 12.2: SVM Hyperparameter Tuning</p>","metadata":{}},{"cell_type":"markdown","source":"SVM classifier hyperparameters are as follows:\r\n\r\n> * __C:__ This hyperparameter controls the trade-off between achieving a low training error and a low testing error. A smaller value of C results in a wider margin and a larger number of misclassified training examples, while a larger value of C results in a narrower margin and fewer misclassified training examples.\r\n>\r\n> * __kernel:__ This hyperparameter defines the type of kernel function used to transform the input data into a higher dimensional space where a linear boundary can be found. Common kernel functions include the linear, polynomial, rbf(radial basis function), sigmoid and precomputed kernels.\r\n>\r\n> * __gamma:__ Kernel coefficient for rbf, poly and sigmoid kernels.\r\n>\r\n> * __degree:__ This hyperparameter is only relevant when using the polynomial kernel. It defines the degree of the polynomial function used to transform the input data.\r\n\r\nThe performance of an SVM classifier can be greatly affected by the choice of hyperparameters, and finding the optimal hyperparameters can help improve the performance of the classifier. Therefore, again we define the hyperparameters grid for search, and then call tune_clf_hyperparameters function to find the optimal values for the SVM hyperparameters that best fit our data:","metadata":{}},{"cell_type":"code","source":"# Weights associated with classes\nclass_weights = [{0:x, 1:1.0-x} for x in np.linspace(0.001,0.5,12)]\n\n# Define the hyperparameter grid to search\nparam_grid = [{'kernel': ['poly'], \n               'degree': [2,3,4,5], \n               'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n               'C': [0.01,0.1,1, 10, 100, 1000],\n               'class_weight': class_weights},\n                  \n              {'kernel': ['rbf','sigmoid'],\n               'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n               'C': [0.01,0.1,1, 10, 100, 1000],\n               'class_weight': class_weights},\n                  \n              {'kernel': ['linear'],\n               'C': [0.01,0.1,1, 10, 100, 1000],\n               'class_weight': class_weights}\n             ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:29:09.921456Z","iopub.execute_input":"2024-11-19T08:29:09.921814Z","iopub.status.idle":"2024-11-19T08:29:09.929315Z","shell.execute_reply.started":"2024-11-19T08:29:09.921782Z","shell.execute_reply":"2024-11-19T08:29:09.928418Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Tuning hyperparameters for Support Vector Machines (SVM) can be time-consuming as it involves training the model multiple times with different hyperparameter values and evaluating their performance to find the optimal set of hyperparameters. After tuning SVM hyperparameters once, we found that the rbf kernel is the optimal kernel for this dataset. Therefore, we limit our param_grid to the rbf kernel to reduce the program's run time:","metadata":{}},{"cell_type":"code","source":"# Weights associated with classes\nclass_weights = [{0:x, 1:1.0-x} for x in np.linspace(0.001,0.5,12)]\n\n# Define the hyperparameter grid to search\nparam_grid = [{'kernel': ['rbf'],\n               'gamma': [0.1, 0.01, 0.001, 0.0001], \n               'C': [0.1, 1, 10, 100, 1000],\n               'class_weight': class_weights}]  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:29:09.930643Z","iopub.execute_input":"2024-11-19T08:29:09.930991Z","iopub.status.idle":"2024-11-19T08:29:09.944952Z","shell.execute_reply.started":"2024-11-19T08:29:09.930959Z","shell.execute_reply":"2024-11-19T08:29:09.944092Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's call tune_clf_hyperparameters function for hyperparameter tuning:","metadata":{}},{"cell_type":"code","source":"# Create a SVC object\nsvm = SVC(probability=True, random_state=0)\n\n# Find the best classifier with the optimal hyperparameters\nsvm_opt = tune_clf_hyperparameters(svm, param_grid, X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:29:09.946443Z","iopub.execute_input":"2024-11-19T08:29:09.946869Z","iopub.status.idle":"2024-11-19T08:37:41.02687Z","shell.execute_reply.started":"2024-11-19T08:29:09.946824Z","shell.execute_reply":"2024-11-19T08:37:41.025643Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 12.3: SVM Feature Subset Selection</p>","metadata":{}},{"cell_type":"markdown","source":"SVM classifiers can be sensitive to irrelevant features. If the input data contains irrelevant features, these features can negatively impact the performance of the SVM classifier. This is because the SVM algorithm considers all features equally when finding the decision boundary, and irrelevant features can distract the algorithm from finding the boundary that separates the classes effectively.\r\n\r\nTo avoid this issue, it is recommended to perform feature selection before training an SVM classifier, which involves removing any irrelevant or redundant features from the input data. This can improve the performance of the SVM classifier by reducing the complexity of the problem and allowing the algorithm to focus on the most relevant features.","metadata":{}},{"cell_type":"code","source":"drop_column_importance_plot(svm_opt, X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:37:41.028513Z","iopub.execute_input":"2024-11-19T08:37:41.028978Z","iopub.status.idle":"2024-11-19T08:38:37.881421Z","shell.execute_reply.started":"2024-11-19T08:37:41.028931Z","shell.execute_reply":"2024-11-19T08:38:37.880138Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the Drop-Column feature importance output results, several features with negative importance are observed. Removing these features improves the performance of the model. Let's filter our the dataset:","metadata":{}},{"cell_type":"code","source":"# Find Important features with positive feature_importance value\nfeature_importances = drop_column_importance(svm_opt, X_train, y_train, 0)\nselected_features = feature_importances[feature_importances['feature importance']>0.01]['feature']  # Threshold value of 0.01\n\n# Filter dataset\nX_train = X_train[selected_features]\nX_test = X_test[selected_features]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:38:37.882751Z","iopub.execute_input":"2024-11-19T08:38:37.883098Z","iopub.status.idle":"2024-11-19T08:39:34.54881Z","shell.execute_reply.started":"2024-11-19T08:38:37.883066Z","shell.execute_reply":"2024-11-19T08:39:34.547926Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's tune the hyperparameters of our model again after removing the irrelevant features:","metadata":{}},{"cell_type":"code","source":"# Create a SVC object\nsvm = SVC(probability=True, random_state=0)\n\n# Find the best classifier with the optimal hyperparameters\nsvm_opt = tune_clf_hyperparameters(svm, param_grid, X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:39:34.550142Z","iopub.execute_input":"2024-11-19T08:39:34.550496Z","iopub.status.idle":"2024-11-19T08:47:11.066029Z","shell.execute_reply.started":"2024-11-19T08:39:34.550465Z","shell.execute_reply":"2024-11-19T08:47:11.06498Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 12.4: SVM Model Evaluation</p>","metadata":{}},{"cell_type":"markdown","source":"Let's evaluate the SVM model performance using model_evaluation function:","metadata":{}},{"cell_type":"code","source":"model_evaluation(svm_opt, X_train, X_test, y_train, y_test, 'SVM')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:47:11.067379Z","iopub.execute_input":"2024-11-19T08:47:11.067703Z","iopub.status.idle":"2024-11-19T08:47:11.839807Z","shell.execute_reply.started":"2024-11-19T08:47:11.067672Z","shell.execute_reply":"2024-11-19T08:47:11.838736Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We achieved the same f1-score for our SVM model compared to KNN model, but on the other hand, the value of AUC has been improved to 0.98!","metadata":{}},{"cell_type":"code","source":"# Save the final performance of SVM classifier\nsvm_result = metrics_calculator(svm_opt, X_test, y_test, 'SVM')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:47:11.841123Z","iopub.execute_input":"2024-11-19T08:47:11.841917Z","iopub.status.idle":"2024-11-19T08:47:11.891918Z","shell.execute_reply.started":"2024-11-19T08:47:11.841851Z","shell.execute_reply":"2024-11-19T08:47:11.890754Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:130%; color:white; text-align:center; border-radius:10px 10px; padding:15px\">Step 13: Decision Tree Model Building</p>","metadata":{}},{"cell_type":"markdown","source":"A Decision Tree is a tree-based supervised learning algorithm that can be used for classification or regression tasks. It works by recursively splitting the data into subsets based on the values of the features, with the goal of minimizing a cost function, such as Gini impurity or entropy.\r\n\r\n<h4 align=\"left\"><font color='midnightblue'>Advantages:</font></h4>\r\n\r\n* __Easy to understand and interpret:__ Decision Trees are simple to understand and visualize.\r\n\r\n* __Handles both numerical and categorical data:__ Decision Trees can handle both numerical and categorical data.\r\n\r\n* __Feature selection:__ Decision Trees can be used for feature selection, as important features will appear near the root of the tree.\r\n\r\n* __Non-parametric:__ Decision Trees are non-parametric, meaning that they make no assumptions about the underlying distribution of the data.\r\n\r\n\r\n<h4 align=\"left\"><font color='midnightblue'>Disadvantages:</font></h4>\r\n\r\n* __Overfitting:__ Decision Trees can be prone to overfitting, especially with deep trees or with small training sets. This can result in poor generalization to new data.\r\n\r\n* __Instability:__ Small changes in the data can result in large changes in the tree, making Decision Trees unstable.\r\n\r\n* __Bias towards features with many categories:__ Decision Trees can be biased towards features with many categories, as they may dominate the construction of the tree.\r\n\r\n* __Poor approximation of complex functions:__ Decision Trees may not be suitable for approximating complex functions, as they are limited by the axis-parallel splits at each node.","metadata":{}},{"cell_type":"markdown","source":"__Note:__ Decision Trees do not require standard scaling of the data before model building. Standard scaling is typically used for algorithms that are sensitive to the scale of the input features, such as distance-based algorithms like KNN and SVM.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 13.1: Decision Tree Hyperparameter Tuning</p>","metadata":{}},{"cell_type":"markdown","source":"__Decision Tree__ classifiers are prone to overfitting. This occurs when the tree is too complex and fits the training data too closely, capturing even the noise in the data.\r\n\r\n\r\n<h4 align=\"left\"><font color='midnightblue'>Ways to prevent overfitting in decision tree classifiers:</font></h4>\r\n\r\n* __Pruning:__ Removing branches of the tree that do not contribute much to the classification.\r\n* __Using ensemble methods:__ Combining multiple decision trees to obtain a more robust model.\r\n* __Limiting tree size:__ Setting a minimum number of samples required to split an internal node or a maximum depth for the tree.\r\n\r\n<h4 align=\"left\"><font color='midnightblue'>The following are the most common hyperparameters for Decision Tree Classifiers:</font></h4>\r\n\r\n>* __Criterion:__ This hyperparameter determines the criterion used to measure the quality of a split. Commonly used criteria are \"Gini Impurity\" and \"Information Gain\".\r\n>\r\n>* __Maximum depth:__ This hyperparameter controls the maximum depth of the tree. The deeper the tree, the more complex it becomes, which can lead to overfitting. Setting a maximum depth can prevent overfitting by limiting the size of the tree.\r\n>\r\n>* __Minimum samples per split:__ This hyperparameter sets the minimum number of samples required to split an internal node. If the number of samples at a node is less than this value, the node cannot be split further. This can also prevent overfitting by limiting the size of the tree.\r\n>\r\n>* __Minimum samples per leaf:__ This hyperparameter sets the minimum number of samples required for a leaf node. If a leaf node has fewer samples than this value, it can be removed.\r\n>\r\n>* __Maximum features:__ This hyperparameter determines the maximum number of features to consider when splitting a node. It is used to prevent overfitting by reducing the complexity of the model.\r\n>\r\n>* __Class_weight:__ Weights associated with classes.\r\n\r\n\r\nAgain we define our hyperparameters grid using the above hyperparameteres and then call tune_clf_hyperparameters function to find the best combination:","metadata":{}},{"cell_type":"code","source":"# Weights associated with classes\nclass_weights = [{0:x, 1:1.0-x} for x in np.linspace(0.001,1,20)]\n    \n# Define the hyperparameter grid\nparam_grid = {'criterion': ['gini', 'entropy', 'log_loss'],\n              'max_depth': np.arange(1, 10),\n              'min_samples_split': np.arange(1, 10),\n              'min_samples_leaf': np.arange(1, 10),\n              'max_features': [None, 'sqrt', 'log2'],\n              'class_weight': class_weights} ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:47:11.893511Z","iopub.execute_input":"2024-11-19T08:47:11.893939Z","iopub.status.idle":"2024-11-19T08:47:11.901113Z","shell.execute_reply.started":"2024-11-19T08:47:11.893892Z","shell.execute_reply":"2024-11-19T08:47:11.899943Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's call tune_clf_hyperparameters function for hyperparameter tuning:","metadata":{}},{"cell_type":"code","source":"# Perform train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n\n# Create a  Decision Tree Classifier object\ndt = DecisionTreeClassifier(random_state=0)\n\n# Find the best classifier with the optimal hyperparameters\ndt_opt = tune_clf_hyperparameters(dt, param_grid, X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T08:47:11.902453Z","iopub.execute_input":"2024-11-19T08:47:11.9028Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 13.2: Decision Tree Feature Subset Selection</p>","metadata":{}},{"cell_type":"markdown","source":"Feature subset selection can be important for a Decision Tree classifier. Removing redundant or irrelevant features can help improve the performance of the model by reducing overfitting, increasing interpretability, and improving computational efficiency. However, the specific importance of feature subset selection depends on the particular problem and dataset being used and in some cases, a Decision Tree model can perform well without any feature subset selection:","metadata":{}},{"cell_type":"code","source":"drop_column_importance_plot(dt_opt, X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the output of  Drop-Column feature importance method, we observe that some features have negative importance. To enhance the performance of the model, we will remove these features:","metadata":{}},{"cell_type":"code","source":"# Find Important features with positive feature_importance value\nfeature_importances = drop_column_importance(dt_opt, X_train, y_train, 0)\nselected_features = feature_importances[feature_importances['feature importance']>0.01]['feature'] # Threshold value of 0.01\n\n# Filter dataset\nX_train = X_train[selected_features]\nX_test = X_test[selected_features]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's tune the hyperparameters of our model again after removing the irrelevant features:","metadata":{}},{"cell_type":"code","source":"# Create a  Decision Tree Classifier object\ndt = DecisionTreeClassifier(random_state=0)\n\n# Find the best classifier with the optimal hyperparameters\ndt_opt = tune_clf_hyperparameters(dt, param_grid, X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 13.3: Decision Tree Model Evaluation</p>","metadata":{}},{"cell_type":"markdown","source":"Let's evaluate our final Decision Tree classifier performance using model_evaluation function:","metadata":{}},{"cell_type":"code","source":"model_evaluation(dt_opt, X_train, X_test, y_train, y_test, 'Decision Tree')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For the decision tree classifier, we got 91% f1-score, similar to the previous two classifiers (KNN & SVM). If we examine the confusion matrix of these models, in all 3 cases, the sum of FN and FP values is equal to 15. In other words, out of 979 bank customers, the models made errors in 15 cases to predict whether the customer accepts the loan or not.","metadata":{}},{"cell_type":"code","source":"# Save the final performance of Decision Tree classifier\ndt_result = metrics_calculator(dt_opt, X_test, y_test, 'Decision Tree')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:130%; color:white; text-align:center; border-radius:10px 10px; padding:15px\">Step 14: Ensemble Learning</p>","metadata":{}},{"cell_type":"markdown","source":"__Ensemble learning__ is a machine learning technique that combines the predictions of multiple models to make more accurate and robust predictions. The idea behind ensemble learning is that by combining several models, or weak learners, to solve the same problem, the resulting ensemble model can perform better than any individual model. There are three main ensemble learning methods: \r\n\r\n* __Bagging__ stands for bootstrapped aggregating. In bagging, multiple instances of the same base model are trained in parallel on different bootstrapped samples of the data, and the results are aggregated through an averaging operation. Bagging is best suited for base models with low bias but high variance, as the averaging operation reduces the variance of the final ensemble model.\r\n\r\n\r\n* __Boosting__ is an iterative technique where multiple instances of the same base model are trained sequentially. At each iteration, the current weak learner is trained based on the previous weak learners and how well they performed on the data. Boosting is best suited for base models with low variance but high bias, as the iterative strategy of learning reduces the bias of the final ensemble model.\r\n\r\n\r\n* __Stacking__ is a technique where different base models are trained independently, and a meta-model is trained on top of that to predict outputs based on the outputs of the base models. In stacking, the base models are used as features for the meta-model, which makes the final predictions based on the combined information from all the base models.","metadata":{}},{"cell_type":"markdown","source":"<h4 align=\"left\"><font color='royalblue'>DTs are often used as base models in ensemble methods because they have several properties that make them well-suited for this purpose:</font></h4>\r\n\r\n1. __Simple and easy to understand:__ Decision trees are simple and easy to understand, which makes them a good choice for use as base models in ensemble methods.\r\n\r\n2. __Handle non-linear relationships:__ Decision trees can handle non-linear relationships between features and target variables, which makes them a good choice for modeling complex datasets.\r\n\r\n3. __Handle missing values and outliers:__ Decision trees are able to handle missing values and outliers in the data, which is important for real-world datasets that often have such problems.\r\n\r\n4. __Can capture interactions between features:__ Decision trees can capture interactions between features, which is important for capturing complex relationships in the data.\r\n\r\n5. __Fast to train and make predictions:__ Decision trees are fast to train and make predictions, which makes them well-suited for use in large-scale machine learning models.\r\n\r\n6. __Provide feature importance:__ Decision trees provide feature importance, which can be useful for understanding which features are contributing the most to the predictions made by the model.","metadata":{}},{"cell_type":"markdown","source":"<h3 align=\"left\"><font color='midnightblue'>The most important bagging models based on decision tree classifier are:</font></h3>\r\n\r\n* __Random Forest:__ Random forests are an ensemble learning method that utilizes decision trees as its base model. The trees in a random forest are grown on bootstrapped samples of the training data and a randomly selected subset of features. This helps reduce the correlation between the trees and makes the model more robust to missing data. The goal of random forests is to lower the variance of the model by combining multiple deep decision trees. The combination of bagging and random feature subspace selection results in a more robust and accurate model compared to individual decision trees.\r\n\r\n\r\n* __Extra Trees:__ Extra trees are an extension of random forests that use random splits instead of optimizing the splits based on information gain or other criteria. This makes extra trees faster to train than random forests and less prone to overfitting.\r\n\r\n<h3 align=\"left\"><font color='midnightblue'>The most important boosting models are:</font></h3>\r\n\r\n* __Adaboost:__ This is one of the earliest and most popular boosting algorithms. It trains a series of weak decision trees and assigns more weight to the misclassified samples in each iteration.\r\n\r\n\r\n* __Gradient Boosting:__ This is a general purpose boosting algorithm that works well with a variety of weak models, including decision trees. It uses the gradient descent optimization method to minimize the loss function and find the best combination of weak models.\r\n\r\n\r\n* __XGBoost:__ This is an optimized implementation of gradient boosting and one of the most widely used algorithms in the machine learning community. It is known for its fast training speed, scalability, and ability to handle large datasets.\r\n\r\n\r\n* __LightGBM:__ This is another optimized implementation of gradient boosting that is designed for large datasets and has been widely adopted in industry.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:130%; color:white; text-align:center; border-radius:10px 10px; padding:15px\">Step 15: Random Forest Model Building</p>","metadata":{}},{"cell_type":"markdown","source":"__Random Forest__ is an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. The trees in a random forest are grown from randomly selected samples of the training data and a subset of the features. It reduces overfitting and improving its stability by combining multiple trees and hence the name \"Forest\".","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 15.1: Random Forest Hyperparameter Tuning</p>","metadata":{}},{"cell_type":"markdown","source":"Tuning the hyperparameters of a Random Forest classifier can improve its performance on a given problem. The hyperparameters control the complexity and behavior of the model, and their values can have a significant impact on the model's accuracy and generalization ability. For example, setting the maximum depth of the trees too high can result in overfitting, while setting it too low can result in underfitting. The same is true for other hyperparameters such as the minimum number of samples required to split a node, or the number of features considered when splitting a node. Tuning the hyperparameters helps to find the best combination of values that result in the best performance on the particular problem being solved. \r\n\r\n<h3 align=\"left\"><font color='midnightblue'>The hyperparameters of a Random Forest classifier are:</font></h3>\r\n\r\n> * __n_estimators:__ The number of trees in the forest.\r\n>\r\n> * __criterion:__ The function to measure the quality of a split. Common criteria include Gini impurity and information gain.\r\n>\r\n> * __max_depth:__ The maximum depth of a tree. This can be used to control the complexity of the model and prevent overfitting.\r\n>\r\n> * __min_samples_split:__ The minimum number of samples required to split an internal node.\r\n>\r\n> * __min_samples_leaf:__ The minimum number of samples required to be at a leaf node.\r\n>\r\n> * __bootstrap:__ Whether or not to sample with replacement when building the trees in the forest.\r\n>\r\n> * __oob_score:__ Whether or not to use out-of-bag samples to estimate the generalization accuracy.\r\n>\r\n> * __class_weight:__ Weights associated with classes.\r\n>\r\n> * __max_features:__ The maximum number of features to consider when splitting a node. This can be set as a number or a float (percentage) or 'sqrt' or 'log2'.","metadata":{}},{"cell_type":"markdown","source":"We set the range of values for each hyperparameter that we want to consider for our Random Forest classifier, and then use tune_clf_hyperparameters function to find the best combination of hyperparameters that provides the best results:","metadata":{}},{"cell_type":"markdown","source":"__Note:__ Trees that compose a forest can either be __shallow__, meaning they have a limited number of branches or levels, or __deep__, meaning they have many branches or levels and are not fully grown. __Deep trees__ have __low bias__ but __high variance__ and, so, are relevant choices for __bagging methods__ that is mainly focused at __reducing variance__. We consider to have deep trees in choosing the range of values for random forest hyperparameters.","metadata":{}},{"cell_type":"code","source":"# Weights associated with classes\nclass_weights = [{0:x, 1:1.0-x} for x in np.linspace(0.001,1,20)]\n\n# Define the hyperparameter grid to search\nparam_grid = {\n    'n_estimators': [50, 100, 150], \n    'max_depth': np.arange(5, 12),\n    'min_samples_split': [1, 2, 3],\n    'min_samples_leaf': [1, 2, 3],\n    'class_weight': class_weights\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's call tune_clf_hyperparameters function for hyperparameter tuning:","metadata":{}},{"cell_type":"code","source":"# Perform train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n\n# Create a random forest classifier object\nrf = RandomForestClassifier(criterion='gini', max_features=None, bootstrap=True, random_state=0)\n\n# Find the best classifier with the optimal hyperparameters\nrf_opt = tune_clf_hyperparameters(rf, param_grid, X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 15.2: Random Forest Feature Subset Selection</p>","metadata":{}},{"cell_type":"markdown","source":"Feature selection is important for random forest classifier because it helps to improve the model's performance, reduce overfitting, and speed up training time by removing irrelevant, redundant or noisy features from the data. Let's call drop_column_importance_plot function:","metadata":{}},{"cell_type":"code","source":"drop_column_importance_plot(rf_opt, X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"No negative values are observed among Drop-column Feature Importances. So, all features are effective in estimating the target and none of them are considered to be detrimental or redundant.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 15.3: Random Forest Model Evaluation</p>","metadata":{}},{"cell_type":"markdown","source":"Let's evaluate our optimal Raandom Forest classifier performance using model_evaluation function:","metadata":{}},{"cell_type":"code","source":"model_evaluation(rf_opt, X_train, X_test, y_train, y_test, 'Primary RF')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If we compare the values of precision, recall and f1-score for class 1 (more important class) on both training and test sets, we see a slight decrease in the scores on the test dataset compared to training, which shows that the model is fairly overfit.\r\nOverfitting of the model is due to its high variance, and in the following, we try to release the model from overfitting by applying a small change in the value of hyperparameters, hoping that the performance of the model will improve on the test data.","metadata":{}},{"cell_type":"markdown","source":"<h4 align=\"left\"><font color='royalblue'>How do each of these Random Forest classifier hyperparameters increase model variance?</font></h4>\r\n\r\n> * __n_estimators__ - Increasing the number of trees in the forest will increase the variance, as more trees in the forest can capture more diverse patterns in the data.\r\n>\r\n> * __max_depth__ - Increasing the maximum depth of a tree allows the tree to capture more complex patterns in the data, increasing its variance.\r\n>\r\n> * __min_samples_split__ - Decreasing the minimum number of samples required to split an internal node will increase the variance as it allows for more splits to occur.\r\n>\r\n> * __min_samples_leaf__ - Decreasing the minimum number of samples required to be at a leaf node will increase the variance as it allows for smaller leaves to form.\r\n>\r\n> * __max_features__ - Increasing the maximum number of features to consider when splitting a node will increase the variance as it allows for more diverse sets of features to be considered when splitting.\r\n>\r\n> * __bootstrap__ - Sampling with replacement when building the trees in the forest will increase the variance as it allows for more diverse sets of samples to be used for each tree.","metadata":{}},{"cell_type":"markdown","source":"Among the optimal values obtained for hyperparameters, we only increase the value of __min_samples_leaf__ from 2 to 6 in order to partially reduce the variance of the model:","metadata":{}},{"cell_type":"code","source":"# Build random forest classifier object considering the obtained optimal values for hyperparameters\nrf_final = RandomForestClassifier(criterion='gini', max_features=None, bootstrap=True,  n_estimators=100, \n                                  max_depth = 9,  min_samples_leaf=6, min_samples_split=2,\n                                  class_weight={0: 0.58, 1: 0.42}, random_state=0)\n                             \n                            \n# Train the final Random Forest model\nrf_final.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's evaluate our Random Forest classifier again:","metadata":{}},{"cell_type":"code","source":"model_evaluation(rf_final, X_train, X_test, y_train, y_test, 'Random Forest')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We got the remarkable 94% f1-score with 99.81% AUC for our Random Forest classifier. Out of 979 bank customers, the model made errors just in 10 cases to predict whether the customer accepts the loan or not.","metadata":{}},{"cell_type":"code","source":"# Save the final performance of Random Forest classifier\nrf_result = metrics_calculator(rf_final, X_test, y_test, 'Random Forest')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:130%; color:white; text-align:center; border-radius:10px 10px; padding:15px\">Step 16: Extra Trees Model Building</p>","metadata":{}},{"cell_type":"markdown","source":"__Extra Trees (Extremely Randomized Trees)__ is an ensemble learning method for classification and regression problems. It is a variation of the popular Random Forest algorithm and uses randomization in the construction of decision trees to create a forest of trees. \r\n\r\nIn a __Random Forest__, each tree in the ensemble is constructed using a random subset of the features and a random subset of the training samples. The final prediction is made by taking an average (for regression) or a majority vote (for classification) of the predictions made by all the trees in the ensemble. But in an Extra Trees classifier, each tree in the ensemble is constructed using a random subset of the training samples and __a random threshold value is used for each feature to split the samples__. This means that the __Extra Trees__ classifier allows for more randomness in the construction of the trees compared to Random Forest, which can make Extra Trees more suitable for certain types of datasets.\r\n\r\nAnother difference between the two is that the Extra Trees classifier tends to have slightly higher variance compared to Random Forest, meaning it can be more prone to overfitting the training data. However, this higher variance can also lead to better performance on certain datasets, making Extra Trees a useful alternative to consider in some cases.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 16.1: Extra Trees Hyperparameter Tuning</p>","metadata":{}},{"cell_type":"markdown","source":"The hyperparameters of Extra Trees classifier are similar to Random Forest. Therefore, we again set the range of values for each hyperparameter that we want to consider for our Extra Trees classifier, and then use tune_clf_hyperparameters function to find the best combination of hyperparameters that provides the best results:","metadata":{}},{"cell_type":"markdown","source":"__Note:__ Tuning Random Forest hyperparameters can be time-consuming. This is because finding the optimal values for the hyperparameters involves training several decision tree classifiers with different combinations of hyperparameter values and evaluating their performance, which can be computationally expensive and time-consuming. Therefore, we performed hyperparameter tuning once and then narrowed the range of hyperparameter values in the following param_grid:","metadata":{}},{"cell_type":"code","source":"# Weights associated with classes\nclass_weights = [{0:x, 1:1.0-x} for x in np.linspace(0.001,1,20)]\n\n# Define the hyperparameter grid to search\nparam_grid = {\n    'n_estimators': [70, 100, 150], \n    'max_depth': [10,12,14],\n    'min_samples_split': [1,2,3],\n    'min_samples_leaf': [1,2,3],\n    'class_weight': class_weights\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's call tune_clf_hyperparameters function for hyperparameter tuning:","metadata":{}},{"cell_type":"code","source":"# Perform train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n\n# Create a random forest classifier object\net = ExtraTreesClassifier(criterion='gini', max_features=None, bootstrap=True, random_state=0)\n\n# Find the best classifier with the optimal hyperparameters\net_opt = tune_clf_hyperparameters(et, param_grid, X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 16.2: Extra Trees Feature Subset Selection</p>","metadata":{}},{"cell_type":"markdown","source":"Feature Selection can be important for Extra Trees Classifier as well. Feature selection helps in reducing the dimensionality of the dataset and selecting a subset of relevant features that have the most impact on the model's prediction performance. This can lead to improved model interpretability, reduced overfitting, and faster training times. However, the importance of feature selection for Extra Trees Classifier may vary depending on the specific dataset and problem being solved. It's always a good practice to experiment and evaluate the impact of feature selection on the performance of the model:","metadata":{}},{"cell_type":"code","source":"drop_column_importance_plot(et_opt, X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Again, no negative values are observed among Drop-column Feature Importances. So, all features are effective in estimating the target and none of them are considered to be redundant.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 16.3: Extra Trees Model Evaluation</p>","metadata":{}},{"cell_type":"markdown","source":"Let's evaluate our optimal Extra Trees classifier performance using model_evaluation function:","metadata":{}},{"cell_type":"code","source":"model_evaluation(et_opt, X_train, X_test, y_train, y_test, 'Primary Extra Trees')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The comparison of precision, recall, and f1-score for class 1 between the training and test datasets suggests that the model is slightly overfit. If the values for class 1 show a significant decrease in performance on the test set compared to the training set, it indicates that the model has learned the training data too well and has not generalized well to new unseen data. This overfitting leads to a poor performance on the test set and suggests that the model is not suitable for making accurate predictions on new data.","metadata":{}},{"cell_type":"markdown","source":"Based on the explanation given in section 15.3, by increasing the value of min_samples_leaf from 2 to 3, the model becomes less complex and the variance of the model is reduced:","metadata":{}},{"cell_type":"code","source":"# Build Extra Trees classifier object considering the obtained optimal values for hyperparameters\net_final = ExtraTreesClassifier(criterion='gini', max_features=None, bootstrap=True,  n_estimators=70, \n                                max_depth = 14,  min_samples_leaf=1, min_samples_split=3,\n                                class_weight= {0: 0.95, 1: 0.05}, random_state=0)\n                             \n                            \n# Train the final Extra Trees model\net_final.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's evaluate our Extra Trees classifier again:","metadata":{}},{"cell_type":"code","source":"model_evaluation(et_final, X_train, X_test, y_train, y_test, 'Extra Trees')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As can be seen, the model is no longer overfit and the performance of the model on the test data has improved compared to the previous model. We got the astonishing 95% f1-score with 99.84% AUC for our Extra Trees classifier which is our best classifier till now. Out of 979 bank customers, the model made errors just in 8 cases to predict whether the customer accepts the loan or not.","metadata":{}},{"cell_type":"code","source":"# Save the final performance of Extra Trees classifier\net_result = metrics_calculator(et_final, X_test, y_test, 'Extra Trees')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:130%; color:white; text-align:center; border-radius:10px 10px; padding:15px\">Step 17: AdaBoost Model Building</p>","metadata":{}},{"cell_type":"markdown","source":"__AdaBoost (Adaptive Boosting)__ is a popular boosting ensemble learning algorithm that is used for classification and regression tasks. It combines multiple \"weak\" classifiers to create a strong classifier that makes accurate predictions. The weak classifiers are trained one by one and the algorithm adjusts the weights of the training instances based on their misclassification rates. The idea behind AdaBoost is to focus on the samples that are misclassified by the previous weak classifiers, so that the subsequent weak classifiers can do a better job in classifying those samples. In this way, the algorithm tries to improve the overall accuracy of the classifier by combining the outputs of multiple weak classifiers.\r\n\r\nThe term \"weak classifier\" refers to a simple classifier that is not highly accurate, but when combined with other weak classifiers, results in a strong overall classifier. Typically, decision trees are used as the weak classifiers in AdaBoost. However, any machine learning algorithm that accepts weights on the training data can be used as the base learner in AdaBoost.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 17.1: AdaBoost Hyperparameter Tuning</p>","metadata":{}},{"cell_type":"markdown","source":"Hyperparameter tuning is generally necessary for most machine learning algorithms, including the AdaBoost classifier. It can help to optimize the performance of the model and avoid overfitting or underfitting.\r\n\r\n<h3 align=\"left\"><font color='midnightblue'>The hyperparameters of the AdaBoost classifier include:</font></h3>\r\n\r\n> * __learning rate:__ This parameter determines the contribution of each weak learner in the final prediction. A lower learning rate results in a slower convergence, but a more accurate prediction.\r\n>\r\n> * __number of estimators:__ This is the maximum number of weak learners that will be used to create the final strong classifier. A larger number of estimators can lead to a more accurate prediction, but it also increases the computation time.\r\n>\r\n> * __estimator:__ The algorithm used for the weak learner can be selected based on the problem at hand. We choose Decision Tree.","metadata":{}},{"cell_type":"markdown","source":"__Note:__ In addition to the hyperparameters of the AdaBoost classifier, such as the learning rate, number of estimators, and sampling strategy, the hyperparameters of the base estimator also need to be optimized to achieve the best performance. Shallow trees have high bias but low variance and, so, are relevant choices for boosting methods that are mainly focused at reducing bias. Shallow trees allow the boosting algorithm to focus on samples that are difficult to classify.","metadata":{}},{"cell_type":"markdown","source":"We set the range of values for each hyperparameter that we want to consider for our AdaBoost classifier, and then use tune_clf_hyperparameters function to find the best combination of hyperparameters that provides the best results:","metadata":{}},{"cell_type":"code","source":"# Define the hyperparameter grid for AdaBoost\nada_param_grid = {\n    'base_estimator__max_depth': [3, 5, 7],\n    'base_estimator__min_samples_split': [3, 5, 7],\n    'base_estimator__min_samples_leaf': [1, 2, 3],\n    'n_estimators': [50, 100, 150],\n    'learning_rate': [0.8, 0.9, 1]\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's call tune_clf_hyperparameters function for hyperparameter tuning:","metadata":{}},{"cell_type":"code","source":"# Perform train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n\n# Create the Decision Tree classifier as the base estimator\ndt = DecisionTreeClassifier(criterion='gini', max_features=None, random_state=0)\n\n# Create the AdaBoost classifier using Decision Tree as base estimator\nada = AdaBoostClassifier(base_estimator=dt, random_state=0)\n\n# Find the best AdaBoost classifier with the optimal hyperparameters\nada_opt = tune_clf_hyperparameters(ada, ada_param_grid, X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 17.2: AdaBoost Feature Subset Selection</p>","metadata":{}},{"cell_type":"markdown","source":"Feature selection can be important for an AdaBoost classifier. Feature selection helps reduce the dimensionality of the data, decrease the computational cost, and prevent overfitting. Additionally, it can improve the interpretability of the model and increase the accuracy of the classifier by focusing on the most relevant features. However, the specific impact of feature selection on an AdaBoost classifier can vary depending on the problem and the data:","metadata":{}},{"cell_type":"code","source":"drop_column_importance_plot(ada_opt, X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The result of the feature importance analysis shows that none of the features have negative values for their drop-column importances. This indicates that all of the features are contributing positively to the estimation of the target and none of them can be considered unnecessary or redundant. All of the features are deemed to be effective in predicting the target.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 17.3: AdaBoost Model Evaluation</p>","metadata":{}},{"cell_type":"markdown","source":"Let's evaluate our optimal AdaBoost classifier performance using model_evaluation function:","metadata":{}},{"cell_type":"code","source":"model_evaluation(ada_opt, X_train, X_test, y_train, y_test, 'Primary AdaBoost')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Performance metrics for the minority class on the test data show that the model is not overfitting completely, but may not be the best model for the problem. Overfitting of the classifier is due to its high variance, and to avoid overfitting, we can try regularization techniques, which means applying a small change in the value of hyperparameters to reduce the variance of the classifier.","metadata":{}},{"cell_type":"markdown","source":"<h4 align=\"left\"><font color='royalblue'>How to reduce our AdaBoost classifier variance by tuning model hyperparameters?</font></h4>\r\n\r\n> * __Decrease the number of trees in the model:__ AdaBoost uses multiple weak learners (in this case decision trees) to build a strong model. Decreasing the number of trees in the model can reduce model variance.\r\n>\r\n> * __Decrease the maximum depth of the decision trees:__ Decreasing the maximum depth of the decision trees can reduce model variance.\r\n>\r\n> * __Increase the minimum number of samples required to split an internal node:__ Increasing the minimum number of samples required to split an internal node can reduce model variance.\r\n>\r\n> * __Increase the minimum number of samples required to be at a leaf node:__ Increasing the minimum number of samples required to be at a leaf node can reduce model variance.\r\n>\r\n> * __Decrease the learning rate:__ The learning rate in AdaBoost determines the weight that each weak learner is given. Decreasing the learning rate can make the model less prone to overfitting.\r\n","metadata":{}},{"cell_type":"markdown","source":"Among the optimal values obtained for hyperparameters, we only decrease the value of learning_rate from 0.9 to 0.8 in order to partially reduce the variance of the model:","metadata":{}},{"cell_type":"code","source":"# Create the Decision Tree classifier as the base estimator\ndt = DecisionTreeClassifier(criterion='gini', max_features=None, random_state=0, max_depth=5, min_samples_leaf=2, min_samples_split=5)\n\n# Create the AdaBoost classifier using Decision Tree as base estimator\nada_final = AdaBoostClassifier(base_estimator=dt, random_state=0, learning_rate=0.8, n_estimators=100)\n\n# Train the final AdaBoost classifier\nada_final.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's evaluate our final AdaBoost classifier:","metadata":{}},{"cell_type":"code","source":"model_evaluation(ada_final, X_train, X_test, y_train, y_test, 'AdaBoost')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The variance of the model is reduced and thus the performance of the model on the test data is improved. Since the difference in scores on the test and training data is not large and the AUC score is high, we can ignore it. The f1-score obtained on the test data is almost the same as the previous bagging models.","metadata":{}},{"cell_type":"markdown","source":"__Note:__ The __AUC (Area Under Curve)__ metric measures the performance of a binary classifier by plotting the true positive rate against the false positive rate and calculating the area under the curve. A model that is not overfit will have a high AUC, close to 1, indicating that it has a good balance between true positive and false positive predictions. A model that is overfitting, on the other hand, may have a high accuracy on the training set but a lower AUC on the validation set, as it may be making many false positive predictions. Hence, a high AUC score suggests that the model is not overfitting and has good generalization performance on new, unseen data.","metadata":{}},{"cell_type":"code","source":"# Save the final performance of AdaBoost classifier\nada_result = metrics_calculator(ada_final, X_test, y_test, 'AdaBoost')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:130%; color:white; text-align:center; border-radius:10px 10px; padding:15px\">Step 18: Gradient Boosting Model Building</p>","metadata":{}},{"cell_type":"markdown","source":"__Gradient Boosting__ is an ensemble machine learning technique for classification and regression problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise manner, where each subsequent model tries to correct the mistakes of the previous model. The algorithm iteratively trains decision trees in such a way that the trees are able to fit the negative gradient of the loss function being optimized. The final prediction is made by combining the outputs of all the individual trees. \r\n\r\n__Note :__ The implementation of Gradient Boosting Classifier in scikit-learn's ensemble module (sklearn.ensemble.GradientBoostingClassifier) is based on decision tree as the base estimator. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 18.1: Gradient Boosting Hyperparameter Tuning</p>","metadata":{}},{"cell_type":"markdown","source":"Tuning the hyperparameters of Gradient Boosting is important. The hyperparameters can significantly impact the model performance, and finding the optimal set of hyperparameters is crucial for achieving good results. An inappropriate setting of hyperparameters can lead to underfitting or overfitting, and affect the model's ability to generalize to unseen data.\r\n\r\n<h3 align=\"left\"><font color='midnightblue'>The hyperparameters of Gradient Boosting Classifier include:</font></h3>\r\n\r\n> * __n_estimators:__ The number of trees in the ensemble.\r\n>\r\n> * __learning_rate:__ The learning rate shrinks the contribution of each tree by a factor of learning_rate. It is a parameter to control the magnitude of update, and the value should be set lower for a large number of trees.\r\n>\r\n> * __max_depth:__ The maximum depth of the individual decision trees. The deeper the tree, the more splits it has and the more complex the model becomes.\r\n>\r\n> * __min_samples_split:__ The minimum number of samples required to split an internal node.\r\n>\r\n> * __min_samples_leaf:__ The minimum number of samples required to be at a leaf node.\r\n>\r\n> * __max_features:__ The number of features to consider when looking for the best split.\r\n>\r\n> * __subsample:__ The fraction of samples to be used for fitting the individual base learners.\r\n>\r\n> * __loss:__ The loss function to be optimized. The default loss function is ‘deviance’ which refers to logistic regression for binary classification and multinomial deviance for multi-class classification problems.\r\n>\r\n> * __criterion:__ The function to measure the quality of a split. Supported criteria are “friedman_mse” for mean squared error, which is used for regression problems, and “entropy” or “gini” for the information gain, which is used for classification problems.","metadata":{}},{"cell_type":"markdown","source":"We again set the range of values for each hyperparameter that we want to consider for our Gradient Boosting classifier, and then use tune_clf_hyperparameters function to find the best combination of hyperparameters that provides the best results:","metadata":{}},{"cell_type":"code","source":"# Define the hyperparameter grid for tuning\ngbc_param_grid = {\n    'n_estimators': [50, 100, 200, 300, 400, 500],\n    'max_depth': [1, 2, 3, 4, 5],\n    'min_samples_split': [2, 4, 6, 8, 10],\n    'min_samples_leaf': [1, 2, 3, 4, 5],\n    'max_features': [None, 'sqrt', 'log2'],\n    'loss': ['deviance', 'exponential'],\n    'criterion': ['friedman_mse', 'squared_error'],\n    'subsample': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since hyperparameter tuning on the above grid can be very time-consuming due to the large size of the grid, after executing it at once and knowing the optimal values , we make the set of grid values smaller:","metadata":{}},{"cell_type":"code","source":"# Define the hyperparameter grid for tuning\ngbc_param_grid = {\n    'n_estimators': [50, 100, 150],\n    'max_depth': [4, 5, 6],\n    'min_samples_split': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'subsample': [0.9, 1.0],\n    'learning_rate': [0.3, 0.4, 0.5]\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's call tune_clf_hyperparameters function for hyperparameter tuning:","metadata":{}},{"cell_type":"code","source":"# Perform train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n\n# Initialize the Gradient Boosting Classifier\ngbc = GradientBoostingClassifier(max_features=None, loss='deviance', criterion='friedman_mse', random_state=0)\n\n# Find the best hyperparameters from the tuning process\ngbc_opt = tune_clf_hyperparameters(gbc, gbc_param_grid, X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 18.2:  Gradient Boosting Feature Subset Selection</p>","metadata":{}},{"cell_type":"markdown","source":"The specific impact of feature selection on a gradient boosting classifier can vary depending on the problem and the data. In some cases, gradient boosting algorithms can handle a large number of features without significant degradation in performance, but in other cases, feature selection can still be beneficial.","metadata":{}},{"cell_type":"code","source":"drop_column_importance_plot(gbc_opt, X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The feature importance analysis reveals that every feature has a positive impact on predicting the target, meaning none of them have a negative impact.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 18.3: Gradient Boosting Model Evaluation</p>","metadata":{}},{"cell_type":"markdown","source":"Let's evaluate our optimal Gradient Boosting classifier performance using model_evaluation function:","metadata":{}},{"cell_type":"code","source":"model_evaluation(gbc_opt, X_train, X_test, y_train, y_test, 'Primary Grad. Boosting')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Our Gradient Boosting classifier is fairly overfit on the minority class. Therefore, agian we need to reduce the variance of the classifier by slightly changing the value of hyperparameters.","metadata":{}},{"cell_type":"markdown","source":"<h4 align=\"left\"><font color='royalblue'>How to reduce our Gradient Bossting classifier variance by tuning model hyperparameters?</font></h4>\r\n\r\n> * __Number of Trees:__ We can reduce the number of trees to reduce model variance. More trees means more complex models, which are more prone to overfitting.\r\n>\r\n> * __Maximum Depth of Trees:__ We can reduce the maximum depth of trees in the model, which will reduce model variance by limiting the number of splits.\r\n>\r\n> * __Learning Rate:__ Decreasing the learning rate will reduce the magnitude of the updates to the model parameters, making it harder for the model to overfit the training data.\r\n>\r\n> * __Subsampling:__ can subsample the training data by taking a smaller sample of it during each iteration of the model training. This will prevent overfitting by introducing randomness into the model.\r\n>\r\n> * __Tree-Specific Hyperparameters:__ We can reduce the variance of the model by adjusting hyperparameters specific to decision trees. For exmale we can increase the minimum number of samples required to split a node or the minimum number of samples in a leaf.","metadata":{}},{"cell_type":"markdown","source":"Among the optimal values obtained for hyperparameters, we only decrease the value of learning_rate from 0.4 to 0.2 in order to partially reduce the variance of the model:","metadata":{}},{"cell_type":"code","source":"# Initialize the Gradient Boosting Classifier\ngbc_final = GradientBoostingClassifier(max_features=None, loss='deviance', criterion='friedman_mse',\n                                 learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0,\n                                 min_samples_leaf=4, min_samples_split=2, random_state=0)\n\n# Train the final AdaBoost classifier\ngbc_final.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's evaluate our final Gradient Boosting classifier:","metadata":{}},{"cell_type":"code","source":"model_evaluation(gbc_final, X_train, X_test, y_train, y_test, 'Gradient Boosting')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The variance of the model is reduced and thus we got the brilliant 95.24% f1-score with 99.92% AUC for our Gradient Boosting classifier which is the best performance among all the classifiers reviewed till now. Out of 979 bank customers, the model made errors just in 8 cases to predict whether the customer accepts the loan or not. Let us unveil our ultimate weapon now: __XGBoost Classifier !__","metadata":{}},{"cell_type":"code","source":"# Save the final performance of Gradient Boosting classifier\ngbc_result = metrics_calculator(gbc_final, X_test, y_test, 'Gradient Boosting')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:130%; color:white; text-align:center; border-radius:10px 10px; padding:15px\">Step 19: XGBoost Model Building</p>","metadata":{}},{"cell_type":"markdown","source":"__XGBoost__ is a type of gradient boosting algorithm for tree-based machine learning models. It stands for __eXtreme Gradient Boosting__. XGBoost is a highly optimized implementation of gradient boosting and is designed to be fast and memory efficient.\r\n\r\n<h4 align=\"left\"><font color='midnightblue'>XGBoost has several unique features compared to other gradient boosting implementations, such as:</font></h4>\r\n\r\n* Handling missing values\r\n* Parallel processing for training and prediction\r\n* Tree pruning for reducing overfitting\r\n* Regularization for preventing overfitting\r\n\r\nThese features make XGBoost a popular and powerful tool for solving many machine learning problems","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 19.1: XGBoost Hyperparameter Tuning</p>","metadata":{}},{"cell_type":"markdown","source":"__XGBoost (eXtreme Gradient Boosting)__ is an open-source library for gradient boosting that is widely used for classification and regression problems. It stands for Extreme Gradient Boosting and is an implementation of gradient boosting trees that is optimized for speed and performance. XGBoost is a highly flexible algorithm that allows users to define custom objectives and evaluation criteria and handles missing values efficiently. \r\n\r\n<h3 align=\"left\"><font color='midnightblue'>XGBoost Advantages:</font></h3>\r\n\r\n* __Regularization:__ Unlike the standard GBM implementation, XGBoost has regularization which helps to reduce overfitting.\r\n\r\n* __Fast Parallel Processing:__ XGBoost implements parallel processing, making it much faster than GBM. It also supports Hadoop implementation.\r\n\r\n* __High Flexibility:__ XGBoost allows users to define custom optimization objectives and evaluation criteria, providing a new level of customization.\r\n\r\n* __Handling Missing Values:__ XGBoost has an in-built routine for handling missing values and can learn how to handle them in future predictions.\r\n\r\n* __Effective Tree Pruning:__ XGBoost makes splits up to the specified maximum depth and then prunes the tree, whereas GBM stops splitting when it encounters a negative loss.\r\n\r\n* __Built-in Cross-Validation and Continued Training:__ XGBoost allows for cross-validation during each boosting iteration, making it easier to determine the optimum number of iterations. Additionally, it can start training from its last iteration of a previous run.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 19.1: XGBoost Hyperparameter Tuning</p>","metadata":{}},{"cell_type":"markdown","source":"Building a model with XGBoost is easy, but improving the model requires hyperparameter tuning. This involves adjusting specific settings to optimize the model's performance. Determining the right set of hyperparameters and their ideal values is challenging and requires careful experimentation and analysis. Despite the difficulties in fine-tuning the hyperparameters, XGBoost remains a highly effective solution for predictive modeling.\r\n\r\n<h3 align=\"left\"><font color='midnightblue'>The most common hyperparameters used in XGBoost:</font></h3>\r\n\r\n>* __eta (Learning rate):__ Step size shrinkage used in updates to prevent overfitting.\r\n>\r\n>* __max_depth:__ Maximum tree depth for base learners. Increasing this value will make the model more complex and more likely to overfit.\r\n>\r\n>* __gamma:__ Minimum loss reduction required to make a further partition on a leaf node of the tree.\r\n>\r\n>* __lambda (reg_lambda):__ L2 regularization term on weights. Increasing this value will make the model more conservative.\r\n>\r\n>* __alpha (reg_alpha):__ L1 regularization term on weights. Increasing this value will force more feature selection.\r\n>\r\n>* __subsample:__ Subsample ratio of the training instances. Setting it to a value less than 1 will make the model more random.\r\n>\r\n>* __colsample_bytree:__ Subsample ratio of columns when constructing each tree.\r\n>\r\n>* __colsample_bylevel:__ Subsample ratio of columns for each level.\r\n>\r\n>* __n_estimators:__ Number of trees in the forest.\r\n>\r\n>* __max_leaf_nodes:__ Maximum number of terminal nodes or leaves in a tree.\r\n>\r\n>* __max_delta_step:__ Maximum delta step for each leaf. It's used for further control over the range of values of weight for each instance.\r\n>\r\n>* __scale_pos_weight:__  It is the ratio of number of negative class to the positive class and controls the balance of positive and negative weights, useful for unbalanced classes.\r\n>\r\n>* __min_child_weight:__ Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, the building process will give up further partitioning.\r\n>\r\n>* __booster:__ The underlying model used by XGBoost, either tree-based (gbtree) or linear (gblinear). Tree-based booster always outperforms the linear booster and thus the later is rarely used.\r\n>\r\n>* __Objective:__ Defines the loss function used to evaluate the performance of the model.\r\n>\r\n>* __eval_metric:__ Specifies the metric used to judge the performance of the model during training and testing.","metadata":{}},{"cell_type":"markdown","source":"We can again set the range of values for each hyperparameter that we want to consider for our XGBoost classifier, and then use tune_clf_hyperparameters function to find the best combination of hyperparameters that provides the best results:","metadata":{}},{"cell_type":"code","source":"# Perform train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Define the hyperparameter grid:","metadata":{}},{"cell_type":"code","source":"# Define imbalance ratio\nratio = sum(y_train==0)/sum(y_train==1) \n\n# Define the hyperparameter grid to search\nxgb_param_grid = {\n    'max_depth': [5, 6, 7],\n    'learning_rate': [0.1, 0.2, 0.3],\n    'n_estimators': [50, 100, 200],\n    'min_child_weight': [1, 5, 10],\n    'scale_pos_weight': [ratio, ratio*1.3, ratio*1.5],\n    'subsample': [0.6, 0.8, 1.0],\n    'colsample_bytree': [0.6, 0.8, 1.0],\n    'colsample_bylevel': [0.6, 0.8, 1.0],\n    'reg_alpha': [0, 0.1, 1],\n    'reg_lambda': [0, 0.1, 1],\n    'max_delta_step': [0, 1, 2],\n    'gamma': [0, 0.1, 1],\n    'max_leaf_nodes': [2, 4, 6]\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Due to the large number of XGBoost hyperparameters, the process of setting hyperparameters will be very time-consuming. For this reason, we made the set of grid values smaller and finally we got the following combination of hyperparameters for our XGBoost classifier:","metadata":{}},{"cell_type":"code","source":"# Initialize the XGBoost Classifier\nxgb_opt = XGBClassifier(max_depth=5,\n                        learning_rate=0.3,\n                        n_estimators=200,\n                        min_child_weight=1,\n                        scale_pos_weight=1.5,\n                        colsample_bytree=0.8,\n                        gamma=0.1,\n                        booster='gbtree',\n                        objective='binary:logistic',\n                        eval_metric='error', \n                        random_state=0)\n\n# Train the XGBoost Classifier\nxgb_opt.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Determining the optimal set of hyperparameters in XGBoost is very time-consuming, so we didn't delve too deep into it!","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 19.2:  XGBoost Feature Subset Selection</p>","metadata":{}},{"cell_type":"markdown","source":"Feature subset selection is important for XGBoost classifier as well. Feature subset selection helps to reduce the complexity of the model, reduce overfitting, and improve the accuracy of the model. By selecting the most relevant features, XGBoost can focus on the most important information and make more accurate predictions:","metadata":{}},{"cell_type":"code","source":"drop_column_importance_plot(xgb_opt, X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The results of the feature importance analysis show that some of the features in the dataset contribute negatively towards making accurate predictions for the target variable. Therefore, we remove these features from our dataset:","metadata":{}},{"cell_type":"code","source":"# Find Important features\nfeature_importances = drop_column_importance(xgb_opt, X_train, y_train, 0)\nselected_features = feature_importances[feature_importances['feature importance']>0.002]['feature'] # Threshold value of 0.002\n\n# Filter dataset\nX_train = X_train[selected_features]\nX_test = X_test[selected_features]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's train our model again after removing the irrelevant features:","metadata":{}},{"cell_type":"code","source":"# Initialize the XGBoost Classifier\nxgb = XGBClassifier(max_depth=5,\n                    learning_rate=0.3,\n                    n_estimators=200,\n                    min_child_weight=1,\n                    scale_pos_weight=1.5,\n                    colsample_bytree=0.8,\n                    gamma=0.1,\n                    booster='gbtree',\n                    objective='binary:logistic',\n                    eval_metric='error', \n                    random_state=0)\n\n# Train the XGBoost Classifier\nxgb.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:90%; color:white; text-align:center; border-radius:10px 10px; padding:10px\">Step 19.3: XGBoost Model Evaluation</p>","metadata":{}},{"cell_type":"markdown","source":"Let's evaluate our final XGBoost classifier performance using model_evaluation function:","metadata":{}},{"cell_type":"code","source":"model_evaluation(xgb, X_train, X_test, y_train, y_test, 'Primary XGBoost')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The XGBoost classifier is fairly overfitting on the minority class and the hyperparameters need to be adjusted to reduce the variance and prevent overfitting.","metadata":{}},{"cell_type":"markdown","source":"<h4 align=\"left\"><font color='royalblue'>How to reduce our XGBoost classifier variance?</font></h4>\r\n\r\n> * __min_child_weight:__ We can increase the value of min_child_weight to control the complexity of the tree model and prevent overfitting.\r\n>\r\n> * __max_depth:__ We can decrease the max_depth to prevent the model from learning too much from the training data and reduce model variance.\r\n>\r\n> * __gamma:__ We can increase the value of gamma to control the minimum loss reduction required to make a split in order to reduce model variance.\r\n>\r\n> * __lambda:__ We can increase the value of lambda to add regularization and control overfitting.\r\n>\r\n> * __subsample:__ We can decrease the value of subsample to reduce the number of samples used to fit each tree and reduce model variance.\r\n>\r\n> * __colsample_bytree:__ We can decrease the value of colsample_bytree to reduce the number of features used in each tree and avoid overfitting.\r\n>\r\n> * __n_estimators:__ we can increase the number of trees to reduce the variance of the model.","metadata":{}},{"cell_type":"markdown","source":"Among the optimal values obtained for hyperparameters, we only decrease the value of max_depth from 5 to 4 in order to partially avoid overfitting:","metadata":{}},{"cell_type":"code","source":"# Initialize the XGBoost Classifier\nxgb_final = XGBClassifier(max_depth=4,\n                          learning_rate=0.3,\n                          n_estimators=200,\n                          min_child_weight=1,\n                          scale_pos_weight=1.5,\n                          colsample_bytree=0.8,\n                          gamma=0.1,\n                          booster='gbtree',\n                          objective='binary:logistic',\n                          eval_metric='error', \n                          random_state=0)\n\n# Train the XGBoost Classifier\nxgb_final.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's evaluate our final XGBoost classifier:","metadata":{}},{"cell_type":"code","source":"model_evaluation(xgb_final, X_train, X_test, y_train, y_test, 'XGBoost')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"XGBoost classifier was our ultimate weapon to deal with our imbalanced dataset. As can be seen, we got the astonishing 97% f1-score with 99.87% AUC for our XGBoost classifier which is the best performance among all the classifiers reviewed. Out of 979 bank customers, the model made errors just in 5 cases to predict whether the customer accepts the loan or not.","metadata":{}},{"cell_type":"code","source":"# Save the final performance of XGBoost classifier\nxgb_result = metrics_calculator(xgb_final, X_test, y_test, 'XGBoost')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"import\"></a>\r\n# <p style=\"background-color:midnightblue; font-family:calibri; font-size:130%; color:white; text-align:center; border-radius:10px 10px; padding:15px\">Step 20: Conclusion</p>","metadata":{}},{"cell_type":"markdown","source":"__As we explained earlier, the most important metric for this project is the f1-score for class '1'. A high f1-score indicates a balance between identifying as many potential loan customers as possible (high recall) and minimizing the number of false positives (high precision), which is crucial for the bank to increase the conversion rate of depositors to borrowers while reducing the cost of the marketing campaign.__\r\n\r\nNext, we can check the performance of all previous classifiers based on metrics:","metadata":{}},{"cell_type":"code","source":"# Concatenate previous classifiers perfermance results into a single dataframe\nresults = pd.concat([cnb_result, bnb_result, logreg_result, knn_result, svm_result, dt_result,\n           rf_result, et_result, ada_result, gbc_result, xgb_result], axis=1).T\n\n# Sort the dataframe in descending order based on F1-score values\nresults.sort_values(by='F1-score', ascending=False, inplace=True)\n\n# Color the F1-score column\nresults.style.applymap(lambda x: 'background-color: royalblue', subset='F1-score')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the following, we can see the f1-score for class '1' of all previous classifiers in the form of a bar chart:","metadata":{}},{"cell_type":"code","source":"# Prepare values\nresults.sort_values(by='F1-score', ascending=True, inplace=True)\nf1_scores = results['F1-score'].str.strip('%').astype(float)\n\n# Plot the barh chart\nfig, ax = plt.subplots(figsize=(10, 8), dpi=70)\nax.barh(results.index, f1_scores, color='royalblue')\n\n# Annotate the values and indexes\nfor i, (value, name) in enumerate(zip(f1_scores, results.index)):\n    ax.text(value+0.5, i, f\"{value}%\", ha='left', va='center', fontweight='bold', color='midnightblue')\n    ax.text(10, i, name, ha='left', va='center', fontweight='bold', color='white', fontsize=18)\n\n# Remove yticks\nax.set_yticks([])\n\n# Set x-axis limit\nax.set_xlim([0,110])\n\n# Add title and xlabel\nplt.title(\"F1-score for class '1'\", fontweight='bold', fontsize=22)\nplt.xlabel('Percentages (%)', fontsize=16)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"🏆 __Among all the tested classifiers, 'XGBoost Classifier' had the best performance in identifying potential loan customers:__\r\n\r\n* __Accuracy = 99.49%__\r\n* __F1-score = 97.08%__\r\n* __Precision = 98.81%__\r\n* __Recall = 95.4%__\r\n* __AUC = 99.87%__\r\n\r\n\r\n🏆 __Based on previous feature importance graphs, among all features, Education, Income, Family, CCAvg and CD Account play the most important role in identifying potential loan customers.__","metadata":{}}]}